{"cells":[{"cell_type":"markdown","source":"# AutoDiff Documentation\n\n\n## Sections:\n\n[I. Introduction](#Introduction)\n\n[II. Background](#Background)\n\n[III. Software Organization](#Software_Organization)\n\n[IV. How to Use](#How_to_Use)\n\n[V. Implementation](#Implementation)\n\n[VI. Testing](#Testing)\n\n[VII. Implemented Extensions](#Implemented_Extensions)\n\n[VIII. Future Features](#Future_Features)\n\n[IX. References](#References)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00000-9da67b0a-600f-43ab-89f7-04249b05a44d"}},{"cell_type":"markdown","source":"<a id='Introduction'></a>\n## I. Introduction","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00000-dde71da3-7c5f-43ce-9ac3-3de3ccafd3bd"}},{"cell_type":"markdown","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-967d32af-aaca-45f2-8703-29d2290297b8","output_cleared":false,"source_hash":"da8d2ffb","execution_start":1605826856856,"execution_millis":5},"source":"The purpose of our software and of automatic differentiation is to efficiently evaluate derivatives at machine precision.\nThere are two main alternatives to automatic differentiation (\"AD\") for the evaluation of derivatives: symbolic differentiation and numerical (approximation) methods.\nWhile symbolic differentiation evaluates derivatives to machine precision, it is often costly to evaluate and prone to 'expression swell': \nsymbolic differentiation may result in exponentially large expressions to evaluate. In contrast, numerical methods (such as the finite difference method) are \nquick to evaluate but do not generally result in machine precision: when evaluating $(f(x+\\epsilon)-f(x))/\\epsilon$, use of a large epsilon results in imprecise approximation,\nwhereas use of a small epsilon results in floating point errors. Moreover, the complexity of numerical methods increases as the number of dimensions of the gradient increases.<sup>3</sup>\nAutomatic differentiation circumvents these challenges by \"accumulating\" numerical evaluations of simple elementary functions.\n    \nThe efficient evaluation of derivatives at machine precision naturally has a wide (and growing) range of application areas, of which a few are highlighted below:\n* Differentiation of functions implemented as code: AD does not require closed-form expressions (as is the case in symbolic differentiation)<sup>3</sup>\n* Backpropagation: training neural networks requires finding weights that minimize the objective function\n    * AD has seen increasingly widespread use since ~2015 as deep learning grows in popularity (e.g. AD implementations in TensorFlow and PyTorch)<sup>3</sup>\n    * Applications include computer vision, NLP\n* Solving ODEs\n* Gradient-based optimization (finding minima through step-wise updates)<sup>3</sup>\n* Root finding, including Newton's method"},{"cell_type":"markdown","source":"<a id='Background'></a>\n## II. Background","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-3910bce6-fca3-4a1c-bc00-15248a4ad565"}},{"cell_type":"markdown","source":"Suppose we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n\nand suppose that we seek ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$.\n\nIf we have $J_f$, the Jacobian matrix of $f$, we can compute ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ as the product of the Jacobian matrix and the desired seed vector $s$:\n\n$$\nJ_f\\mathbf{s}= \\begin{bmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\[1ex]\n    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\[1ex]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n    \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\[1ex]\n0 \\\\[1ex]\n\\vdots \\\\[1ex]\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} \\\\[1ex]\n\\frac{\\partial f_2}{\\partial x_1} \\\\[1ex]\n\\vdots \\\\[1ex]\n\\frac{\\partial f_m}{\\partial x_1}\n\\end{bmatrix}\n$$\n\nRather than using matrix-vector multiplication or an explicit matrix representation of the Jacobian, the forward mode of AD involves calculating ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ \n(i.e.  a single column of the Jacobian) by computing ${\\frac{\\partial f_i}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ for each of $i=1,...,m$ separately in one or more *forward \"passes\"*\n(also sometimes referred to as \"sweeps\").<sup>2,3</sup> If $f: \\mathbb{R}^1 \\to \\mathbb{R}^m$, only one forward pass would be needed to calculate the full Jacobian, since in that case $n=1$, the Jacobian matrix is $m$x$1$,\nand the seed vector would have a single entry. If $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then $n$ forward passes would be required (one forward pass for each entry in the $n$x$1$ seed vector). \nAlthough the matrix representation of the Jacobian is not explicitly used in the forward mode, this intuition carries over: within each forward pass, the derivative of\none of the variables is set to 1 and the rest to zero (for example, $\\dot{x}_1=1$ if we seek ${\\frac{\\partial f}{\\partial x_1}}$)\n\nAD works by breaking down the function $f$ into a composition of much simpler elemental functions (or elemental operations).\nEach forward pass starts by assigning 1 to the derivative of the variable $x_i$ for which we want to differentiate $f$ with respect to, and assigning 0 to all other variables.\nThen the elemental functions are evaluated from innermost to outermost, accumulating two numerical values at each intermediate variable $v_i$ in the trace<sup>3,4</sup>:\n* the **\"primal\"**: the value of the elemental function at $\\mathbf{x}=\\mathbf{a}$, denoted $v_i$ (i.e. the input of the elemental function will be carried over from any inner functions evaluated earlier in the trace)\n* the **derivative** or \"tangent\", denoted $\\dot{v_i}$ (i.e. the derivative of the elemental function evaluated at $\\mathbf{x}=\\mathbf{a}$)\n\nWithin each step, the derivative $\\dot{v_i}$ is implicitly calculated using the chain rule. For example, if the elemental function corresponding to $v_3$ results from the composition of several functions $w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x}))$ which for clarity we denote $w_3(u_1, u_2)$ then:\n$$\n\\dot{v_3} = {\\frac{\\partial}{\\partial x_1}} w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x})) = {\\frac{\\partial w_3}{\\partial u_1}}{\\frac{\\partial w_2}{\\partial x_1}} + {\\frac{\\partial w_3}{\\partial u_2}}{\\frac{\\partial w_1}{\\partial x_1}}\n$$\nIn forward mode, the innermost elemental function primals and derivatives are evaluated first and 'accumulated' outwards.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-82e12561-8e3d-4ac1-9855-f734f94b3ba2"}},{"cell_type":"markdown","source":"<a id='Software_Organization'></a>\n## III. Software Organization","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00005-9f8541e7-5b00-4927-9ba4-903ff36946f2"}},{"cell_type":"markdown","source":"The main project directory contains the `config.sh` file. This file has the \n`cmake` and `make install` commands that build the library, check environment, run test suite \nin `AutoDiff`. The `config.sh` file is specified in `.travis.yml`, enabling \nTravisCI to build our tests each time we `git push`. Our test suite also be utilized \nby `CodeCov` to show how much of our code we've hit, which utilizes `AD_COV.info`.\n\nThere are two main ways to package cplusplus software: `cmake` and `make`. We choose `CMake` \nbecause the user has to do so much less in order to link up our library. `Make` requires us to \nconfigure the `VPATH` and locate the `.so` file. `CMake` requires less user configuration. \n\nOur `AutoDiff/` folder is what the user needs to have locally in order to utilize our AutoDiff library.\nTo build, the user will first use \"./config\" in the root directory. It will then generate two folders: \n`build` and `install` under AutoDiff. \nThe user will need to `cd` into the `build/` folder, run the `cmake` command, then edit the `Makefile`, \nadding the `target_link_libraries` command to link up our library. Then, all the user needs to do is \nadd `#include <AutoDiff.h>` at the top of their `.cpp` file, and `C++` will know where to find our \nAutoDiff library.\n\nBelow is a diagram of our directory stucture. \n\n```\ncs107-FinalProject/\n            | README.md\n            | config.sh\n            | docs/\n                | README.md\n                | milestone1.ipynb\n                | milestone2_progress.ipynb\n                | milestone2.ipynb\n            | .travis.yml\n            | App/\n            | Dual/\n                | CMakeLists.txt\n                | config.sh\n                | coverage.sh\n                | include/\n                    | Dual.h\n                | core/\n                    | src/\n                        | Dual.cpp\n                        | CMakeLists.txt\n                    | tests\n                        | CMakeLists.txt\n                        | test_main.cpp\n                        | src/\n                            | CMakeLists.txt\n                            | test_DUAL.cpp\n                            | test_vars.h\n                    | CMakeLists.txt\n            | AutoDiff/\n                | CMakeLists.txt\n                | config.sh\n                | coverage.sh\n                | include/\n                    | AutoDiff.h\n                | core/\n                    | src/\n                        | AutoDiff.cpp\n                        | CMakeLists.txt\n                    | tests\n                        | CMakeLists.txt\n                        | test_main.cpp\n                        | src/\n                            | CMakeLists.txt\n                            | test_AD.cpp\n                            | test_vars.h\n                    | CMakeLists.txt\n```\n\nWe use GoogleTest to generate local code coverage reports. We use `converge.sh` file (in Dual/ and AutoDiff/) to make it \neaiser to call lcov and link coverage info to an `index.html` under a generated \"coverage\" folder. To call\n`converge.sh`, the user may run \"./config.sh -gtest\" in the root directory, or go to Dual/ folder then do \"./coverage.sh\"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-1414827a-21ce-43ed-8201-2b4a09c4c2cb"}},{"cell_type":"markdown","source":"<a id='How_to_Use'></a>\n## IV. How to Use","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-af100369-7877-4b16-ad05-cda12140f003"}},{"cell_type":"markdown","source":"### Configuration:\n\nUnder the root directory, there is a `config.sh` file which is essential to get started this application.\n\nDefault option: `./config.sh`\n\nCleaning: `./config.sh -c`\n\nRun UnitTest: `./config.sh -gtest`\n\nFor help: `./config.sh -h`, and you will see a list of options to configure.\n\n### Package Requirements:\n\n1. cmake\n\ntutorial: https://cliutils.gitlab.io/modern-cmake/chapters/intro/installing.html\n\ndocumentation: https://cmake.org/cmake/help/v3.19/\n\nuseful links: https://github.com/ackirby88/CS107/tree/master/cmake\n\n2. GoogleTest (for testing only, not required)\n\nUnder the root directory, type \"cd 3PL\" in the coomand prompt to go to the 3rd package library. Type \"./build_3PL.sh\" to install googleTest.\n\ntutorial: https://github.com/google/googletest/tree/master/googletest\n\n3. lcov (for testing only, not required)\n\nUnder the root directory, type \"cd 3PL/lcov\" in the coomand prompt to go to the lcov package. Then, type \"make install\" or \"sudo make install\"\nin the command prompt to install it. When installation is completed, you may try \"make test\" to see if it is installed correctly.\n\ntutorial: http://ltp.sourceforge.net/coverage/lcov.php\n\nsource code repo: https://github.com/linux-test-project/lcov\n\n### Usage:\n\nTo use our `AutoDiff library`:\n\n1. the user places `#include<AutoDiff.h>` at the top of their `.cpp` file wherein they intend to compute forward mode.\n\n2. A user build up an object of `double` and `AutoDiff types`. When declaring an `AutoDiff type` object, they must specify the object value, and may optionally seed that value as the second parameter. If only one parameter value is provided, the derivative is seeded with `1.0`.\n\n3. The user then calls `.val` (or `.getVal()`) on their object to obtain the function value and call `der` (or `.getDer()`) on their object to obtain the derivative computed by forward mode.\n\n\n#### Notice: If you get a \"permission denied\", you need to check if config.sh, coverage.sh, or Makefile is executable. Type \"chmod +x \\[executable file\\]\" in the command prompt to add files to be executable.\n\n### Operating system: \n\n1. MacOS\n\nCurrent \"Dual/CMakeLists.txt\" and \"AutoDiff/CMakeLists.txt\" comment out two pieces that make it different from\nLinux version. \n\nThe 1st one is \"line 10: set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib)\". If you receive an error like \"realpath: command not found\", you should consider line10 out.\n\nThe 2nd one is \"line 27: if(CMAKE_C_COMPILER_ID MATCHES \"GNU\")\" and correspondingly \"line 30: endif()\". It maybe due to that MacOS use LLVM gcov to emulate gcov, if MacOS users don't commit these two lines out, the unittesting won't be triggered. Therefore, a code coverage report website cannot be generated.\n\n2. Linux\n\nUncomment the two parts (line10, line27, line30) and it should work. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-a945d4dd-5750-4c41-8be9-6ba7eda2da7b"}},{"cell_type":"markdown","source":"### Usage Examples (Setup)\n\n``git clone https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/m2b-dev``\n\n``cd 3PL``\n\n``./build_3PL.sh``\n\n``cd ..``\n\n``./config.sh ``\n\nif you want to get test coverage, \n\n``./config.sh -gtest``\n\n\n### Usage Examples\nPlease note that currently nested functions are not supported and each step in the trace has to be initialized separately. For example:\n\nFor instance this won’t work:\n```\nAutoDiff test = AutoDiff(4.0);\nAutoDiff test1 = sin(pow(test, 3.0));\n```\n\nBut this works:\n```\nAutoDiff test = AutoDiff(4.0);\nAutoDiff test1 = pow(test, 3.0):\nAutoDiff test2 = sin(test1);\n```\n\nTo get the function evaluation, call `test2.val` or `test2.getVal()`. To get the function derivative by forward mode, call `test2.der` or `test2.getDer()`.\n\nPrint out the evaluation and derivative using `test2.print()`.\n\n\n### External Dependencies\n\nThis project uses the Prof Kirby's skeleton project and `Math.h`.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-e786966a-8968-45a3-9d52-83f3ede7d27a"}},{"cell_type":"markdown","source":"<a id='Implementation'></a>\n## V. Implementation","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-05c10c9e-4531-4794-b16e-de6103e6458d"}},{"cell_type":"markdown","source":"Our `AutoDiff.h` header file contains class declaration and function signature of elemental operations.\n\nOur `AutoDiff class` is realized in `AutoDiff.cpp`, whereby our constructor contains two attributes: double `val` and double `der`: `val` is the value of the `AutoDiff` \"variable\" and `der` is initialized as its seed.\n\nWe overload the following mathematical operators within the `AutoDiff class`:\n```\nAutoDiff AutoDiff::operator + ( AutoDiff &obj )\nAutoDiff AutoDiff::operator - ( AutoDiff &obj )\nAutoDiff AutoDiff::operator * ( AutoDiff &obj )\nAutoDiff AutoDiff::operator / ( AutoDiff &obj )\n```\n\nAnd define the following the following functions which return objects of `AutoDiff type`.\n```\n* AutoDiff operator + (double lhs, AutoDiff &rhs);\n* AutoDiff operator - (double lhs, AutoDiff &rhs);\n* AutoDiff operator * (double lhs, AutoDiff &rhs);\n* AutoDiff operator / (double lhs, AutoDiff &rhs);\n* AutoDiff pow ( AutoDiff &a, AutoDiff &b );\n* AutoDiff pow ( AutoDiff &obj, double a );\n* AutoDiff pow (double lhs, AutoDiff &rhs);\n* AutoDiff exp ( AutoDiff &a );\n* AutoDiff sin(AutoDiff &input);\n* AutoDiff cos(AutoDiff &input);\n* AutoDiff tan(AutoDiff &input);\n* AutoDiff asin(AutoDiff &input);\n* AutoDiff acos(AutoDiff &input);\n* AutoDiff atan(AutoDiff &input);\n* AutoDiff log(AutoDiff &input);\n* AutoDiff sinh(AutoDiff &input);\n* AutoDiff cosh(AutoDiff &input);\n* AutoDiff tanh(AutoDiff &input);\n```\n\nSince each of the functions above accepts AutoDiff objects and returns AutoDiff objects containing \nan 'updated' current value and derivative, the forward mode is realized by stringing together multiple\nfunction calls representing the composition of elemental functions which define the user's function of interest.\nThe output of any given elemental function is effectively the trace.\n\nFor ease of viewing the AutoDiff object, a `print` function is also defined which prints the value and derivative and can be called for example via:\n```\nAutoDiff test = AutoDiff(2.0);\nAutoDiff output = pow(test, 3.0);\noutput.print();\n```\n\n#### Currently, only the single variable case is implemented (i.e. scalar functions of a single input). So, the paragraphs below are our plan, not implemented yet.\n\nNext, we intend to expand our `AutoDiff class` to allow the vector inputs, not just scalars. We will have a second constructor wherein the user must provide a vector (of `duble type`) when declaring their `AutoDiff` object. The second parameter is a seed vector which, if left empty, will seed the vectors as unit vectors (i.e. vectors where the variable in question is `1` and every other variable is `0`).\n\nTo implement evaluation and derivative on a vector input, we intent to expand the overloaded operators and mathematical functions to accept and be able to process vector inputs of `AutoDiff type` (and double). Data structurally, a vector is an array.\n\nAfter we have completed expanding from scalar to vector inputs, we need to be able to calculate evaluation and forward mode on matrices, namely calculating the Jacabian, not just a scalar function.\n\nTo calculate the Jacobian, we will expect the user create a matrix of `AutoDiff type`, requiring us to define such a data structure using a multi-dimesional array. We will create another constructor, overload math operations, and redine math functions. Our constructor will initialize the inputs as multi-dimensional arrays instead of vectors or scalars.\n \nOur implementation is just being scaled to multiple dimensions, where we work with array data types instead of `double` scalar data types.\n\nOnce our AD is implemented within `AutoDiff.cpp` we essentially just need to reference the functionality to write our root finder. The Newton-Raphson method is pretty short, and involves iterating once we acquire the function evaluation and derivative evaluation. The file `roots.cpp` will handle this iteration.\n\nLastly, we will wrap the diff implementation along with the root finder in a file called `AutoDiff_main.cpp`.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-ca37d973-78e5-4288-9ebc-2000825955a3"}},{"cell_type":"markdown","source":"<a id='Testing'></a>\n## VI. Testing","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-e0acdc93-e8bd-4f54-8538-756c116f7b0c"}},{"cell_type":"markdown","source":"* Location：Dual/code/tests and AutoDiff/code/tests\n\n    * We have pre-defined several unit tests under the above directories\n    * To run these tests, please (1) run the AutoDiff/code/config.sh file and (2) run the AutoDiff/code/coverage.sh file\n\n* Tools:\n\n    * CodeCov: \n    \n        CodeCov provides statistics of code coverage so that each commit and pull request is measured and in control. \n        To do this, we need to create a .yaml file and specify our metrics (target at least 90%). \n        As our software covers two parts of functions: the basic function(AD implementation) and extensions, \n        it is reasonable to use tags in the .yaml file to separate coverage reports for different modules<sup>7</sup>. \n        Setting Codecov to send notifications to the Slack channel is also a good idea<sup>8</sup>, so everyone in the group \n        gets notified when there is an update.\n    \n    * TravisCI: \n        \n        TravisCI is a continuous integration platform that automatically builds and tests code changes. \n        Instead of merging large pieces of code and integrate them all in the end, we want to take a continuous \n        integration approach - merge in small code changes and get immediate feedback when each commit and pull \n        request happens<sup>9</sup>. We also need a .yaml file to use it. It can also send notifications through Slack. \n* Plan: \n    * Step 1: Exploratory Testing\n\n        Exploratory testing is a simultaneous process of test design and test execution all done at the same time, \n        contrarily to scripted testing, where all test cases are determined in advance<sup>10</sup>.\n        We will note down ideas about what to test and how it can be tested in a document file (.md). \n        We will update this document with additional test case designs, critically evaluate current test results with \n        respect to our expectations, and learn from the testing constantly.\n\n    * Step 2: Unit tests\n\n        Unit tests isolate issues that may arise and each unit is tested independently. \n        We will define the smallest unit to a each Class method. \n        By writing tests for the smallest testable units, then the compound behaviors between those, \n        we can build up comprehensive tests for more complex applications.<sup>11</sup>\n\n    * Step 3: Acceptance tests\n\n        After unit testings, we want to validate software against functional requirements/specifications. We mainly concentrate on:\n\n        * Functionality\n\n        * Basic Usability: Can users freely navigate through the screens without any difficulties?\n\n        * Error Conditions: Do suitable error messages display? Are they clear and proper?\n\n        * Accessibility\n\n\n        \n\n* Evaluations:\n\n    * Code coverage\n\n    * Risk analysis: What are the potential risks of current implementation? What are the important ones and what need to be addressed? \n\n    * Test execution log: recordings on the test execution","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-a13d467a-3019-46ff-8aa8-d44ae8c58bac"}},{"cell_type":"markdown","source":"<a id='Implemented_Extensions'></a>\n## VII. Implemented Extensions","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-347848c7-f97c-4db9-8af6-64b7e4c63134"}},{"cell_type":"markdown","source":"### Dual Numbers\n\nDual number is an interesting type of number that uses the symbol $\\epsilon$ (epsilon) and offers how to do automatic\ndifferentiation in another persepective. It automatically calculates the derivative and the value of a function at the same time! \n\nDual numbers are pretty similar to imaginary numbers. A dual number has a real part and a dual part. We write\n\n$$\nx = a + b\\epsilon\n$$\n\nBut! For imaginary numbers, $i^2 = -1$, whereas for dual numbers, $\\epsilon^2 = 0$ (and $\\epsilon$ is not 0!). \n\n#### Basic Math\n\nAdd and subtraction of dual numbers are the same as adding complex numbers: we just add the real and dual parts separately.\n\n$$\n(3+6\\epsilon) + (1+2\\epsilon) = (3+1)+(6\\epsilon+2\\epsilon) = 4+8\\epsilon\n$$\n\n$$\n(3+6\\epsilon) - (1+2\\epsilon) = (3-1)+(6\\epsilon-2\\epsilon) = 2+4\\epsilon\n$$\n\nTo multiply dual numbers, we use F.O.I.L. just like we do with complex numbers\n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 6\\epsilon +6\\epsilon + 12\\epsilon^2 = 3 + 12\\epsilon + 12\\epsilon^2\n$$\n\nAs $\\epsilon^2=0$, the last item $12\\epsilon^2$ disappears. \n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 12\\epsilon\n$$\n\nThe division process relates to the conjugate, like complex division - multiplying the denominator  by its conjugate \nin order to cancel the non-real parts.<sup>12</sup> The formula is given below:\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= \\frac{(a+b\\epsilon)(c-d\\epsilon)}{(c+d\\epsilon)(c-d\\epsilon)}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2 - d^2\\epsilon^2}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2}\\\\\n&= \\frac{ac+(bc-ad)\\epsilon}{c^2}\\\\\n&= \\frac{a}{c}+\\frac{(bc-ad)\\epsilon}{c^2}\n\\end{aligned}\n$$\n\nwhich is defined when $c$ is non-zero. If, on the other hand, $c$ is zero while $d$ is not, then the equation\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= k\\\\\n\\frac{a+b\\epsilon}{d\\epsilon} &= k\\\\\na+b\\epsilon &= kd\\epsilon\\\\\n\\end{aligned}\n$$\n\n1. has no solution if $a$ is nonzero\n2. otherwise is solved by any dual number of the form $k=\\frac{b}{d}+y\\epsilon$. To expalins this, suppose we have $k=x+y\\epsilon$: \n\n$$\n\\begin{aligned}\nb\\epsilon &= kd\\epsilon\\\\\nb\\epsilon &= (x+\\epsilon y)d\\epsilon\\\\\nb\\epsilon &= xd\\epsilon + \\epsilon^2 y\\\\\nb\\epsilon &= xd\\epsilon\\\\\nx &= \\frac{b}{d}\\epsilon\n\\end{aligned}\n$$\n\nSo, $k=\\frac{b}{d}+y\\epsilon$ and is the value of $\\frac{a+b\\epsilon}{c+d\\epsilon}$.\n\n#### Using $\\epsilon^2$ for AD\n\nSuppose we have a function: $\nf(x) = 3x + 2$, and calculates $f(4)$ and $f'(4)$. \n\nWe know $4 = 4 + 1\\epsilon$, using $1$ for the dual component, since $x$ has a derivative of $1$.<sup>13</sup> We take this into the function:\n\n$$\nf(4) = f(4 + \\epsilon) = (4 + 1\\epsilon) \\times 3 + 2 + 1\\epsilon = (12 + 3\\epsilon) + 2 = 14 + 3\\epsilon\n$$\n\nWe can find out the real number component $(14)$ is the value of $f(4)$ and the dual component $(3)$ is the derivative $f'(4)$, \nwhich is verified below:\n\n$$\nf(x) = 3x+2 = 3\\times4 + 2 = 14\n$$\n\n$$\nf'(x) = 3\n$$\n\nIt is not an coincidence! Now try a quadratic example: $f(x)=5x^2 + 3x + 1$, also evaluates at $f(4)$.\n\n$$\n\\begin{aligned}\nf(4 + \\epsilon) &=5(4 + \\epsilon)^2 + 3\\times(4 + \\epsilon) + 1 \\\\\n &= 5(16 + 8\\epsilon+\\epsilon^2 ) + 12 + 3\\epsilon + 1\\\\\n &= 80 + 40\\epsilon + 5\\epsilon^2 + 12 + 3\\epsilon + 1\\\\\n &= (80+12+1) + (40+3)\\epsilon + 5\\epsilon^2 \\\\\n &= 93 + 43\\epsilon + 5\\epsilon^2\\\\\n &= 93 + 43\\epsilon\n\\end{aligned}\n$$\n\nNow verify it:\n\n$$\n\\begin{aligned}\nf(4)&=5\\times 4^2 + 3\\times 4 +1\\\\ &= 5\\times 16 + 12 + 1\\\\ &= 80 + 12 + 1 \\\\&= 93\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\nf'(x)&=10x + 3\\\\ \nf'(4)&=10\\times 4 + 3 = 43\n\\end{aligned}\n$$\n\nWe can also use dual number on trigonometric functions to do differentiation. \nWhat's more, it is not only limited to the 1st derivative.\nIt is possible to modify this technique to get the 2nd the 3rd, or the Nth derivative of a function. But in a 2nd derivative\nscenario, we need to define $\\epsilon^3=0$, instead of $\\epsilon^2=0$, so on and so forth.\nWe can also extend this to multivariable functions. To do this, we need to define $\\epsilon_x$, $\\epsilon_y$, ... for each variables.\n\nTo conclude, using dual numbers gives us exact derivatives, within the limitations of floating point math \n(compared to finite difference method). \n\n#### Dual Number Implementation\n\nIt is tempting to create a \"DualNumber\" Class and overloads different operators$(+,-,*,/)$ and math functions, eg. sqrt, pow, sin, cos, tan, atan, ... \nWe keep all independent variables in a array-like container and in each overloading function, we loop through this array to calcualte derivative for each variables.\n\nJust like what stated above, dual number can be expressed as the followings:\n\n$$\nz = a+b\\epsilon\n$$\n$$\n{\\epsilon}^{2} = 0\n$$\n\nFor the purpose of auto differentiation, we can use dual numbers in this way:\n\n$$\nz = x + x^{'}\\epsilon\n$$\n\nNext, we can get a dual number cheat sheet:\n\n| Expr. | Expr. Deri. | Expr. w Dual Num. | Expr. w Dual Num. Result | Real Part | Dual Part | Notes |\n|:-----|:--------------|:-----|:-----------------------|:-------|:-------|\n|  $u+v$| $u^{'}+v^{'}$  |   $(u+u^{'}\\epsilon)+(v+v^{'}\\epsilon)$ | $u+u^{'}\\epsilon+v+v^{'}\\epsilon$  | $u+v$   | $(u^{'}+v^{'})\\epsilon$ |\n|  $u\\times v$     |    $(u^{'}v+vu^{'})$  |   $(u+u^{'}\\epsilon)\\times(v+v^{'}\\epsilon)$    | $uv + u^{'}v\\epsilon+vu^{'}\\epsilon+{\\epsilon}^{2}$   | $u\\times v$  | $(u^{'}v+vu^{'})\\epsilon$ |\n|  $u-v$| $u^{'}-v^{'}$  |   $(u+u^{'}\\epsilon)-(v+v^{'}\\epsilon)$ |   $u+u^{'}\\epsilon-v+v^{'}\\epsilon$| $u-v$  | $(u^{'}-v^{'})\\epsilon$ |\n|  $u\\div v$     |    $\\frac{(u^{'}v+vu^{'})}{v^2}$ |   $(u+u^{'}\\epsilon)\\div(v+v^{'}\\epsilon)$  |  $\\frac{uv+(u^{'}v-uv^{'})\\epsilon}{v^2}$   |   $u\\div v$      |   $\\frac{(u^{'}v-uv^{'})\\epsilon}{v^2}$      |\n| $\\sin(u)$ | $\\cos(u)u^{'}$ | $\\sin(u+u^{'}e)$  | $\\sin(u) +\\cos(u)u^{'}\\epsilon$ | $\\sin(u)$ | $\\cos(u)u^{'}\\epsilon$ | using $\\sin(u'\\epsilon)=u'\\epsilon,\\cos(u^{'}\\epsilon)=1 $\n| $\\cos(u)$ | $-\\sin(u)u^{'}$ | $\\cos(u+u^{'}e)$| $\\cos(u)-\\sin(u)\\epsilon$ |  $\\cos(u)$ | $-\\sin(u)\\epsilon$ |\n| $\\exp(u)$ | $\\exp(u)$ | $\\exp(u+u^{'})$ | $\\exp(u)+u^{'}\\exp(u)$ | $\\exp(u)$ | $u^{'}\\exp(u)$ | using Taylor series\n| $\\log(u)$ | $\\log(u+u^{'})$ | $\\log(u+u^{'}\\epsilon)$ | $\\log(u)+\\frac{u^{'}}{u}\\epsilon$ | $\\log(u)$ | $\\frac{u^{'}}{u}\\epsilon$ |\n| $u^{k}$   | ${ku^{'}u^{k-1}}$ |${(u+u^{'}u^{'}\\epsilon)}^{k}$  | $u^{k}+{ku^{k-1}}\\epsilon$|$u^{k}$ | ${ku^{'}u^{k-1}}$|\n\nSo, we can plan to make a Dual class and use a type name T to let it can be used as float, double ... etc.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-2a36c0e9-f0c7-46bd-9648-23d83135c566"}},{"cell_type":"markdown","source":"<a id='Future Features'></a>\n## VIII. Future Features ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-3774061d-118a-4940-b895-c8de78139edc"}},{"cell_type":"markdown","source":"### Root Finding and Optimization\n\n#### Newton's method (also known as the Newton-Raphson Method):\n\n##### General properties and overview\n* \"second order method\" in that the quadratic approximation for $f$ at a local point $a$ is used to compute the descent direction\n(direction in which to step to reach stationary points)\n* well-suited for optimizing or finding roots of convex functions with a moderate number of inputs (e.g. linear regression, logistic regression)\n* requires fewer steps to converge than methods which use linear (rather than quadratic) approximations of the function\n* with non-convex functions, the algorithm may converge to local maxima\n* as with other local optimization methods, starts from a single starting point and iteratively \"refines\" it by stepping in a descent direction\n* for optimization, the idea is to start at point $\\mathbf{a_i}$ and go to $\\mathbf{a_{i+1}}$ by creating second order Taylor series approx. to $f$ at $a_i$ and traveling to the stationary point of this second order Tayor series approx.<sup>14</sup>\n\n##### Theory\nBuilding from simple to more general cases of the functional form of $f$ provides intuition for Newton's method:<sup>15, 16, 17</sup>\n\n*$f: \\mathbb{R} \\to \\mathbb{R}$*\n$$\nf(x) = f(x)\n$$\n* the second order Taylor approx. of $f$ at $\\mathbf{a}$ is: \n$$g(x) = f(a) + \\frac{d}{dx} f(a) (x-a)+\\frac{1}{2}\\frac{d^2}{dx^2}f(a)(x-a)^2$$\n* we can solve for the stationary point of the second order Taylor approx. of $f$ by setting its derivative to 0:\n$$\n\\begin{aligned}\n\\frac{d}{dx}g = \\frac{d}{dx}f(a) + \\frac{1}{2}\\frac{d^2}{dx^2}f(a)(2x^{*} - 2a) &= 0\\\\\n\\frac{d^2}{dx^2}f(a)(x^{*} - a) &= -\\frac{d}{dx}f(a)\\\\\nx^* &= a - \\frac{\\frac{d}{dx}f(a)}{\\frac{d^2}{dx^2}f(a)}\\\\\n\\end{aligned}\n$$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}$*\n$$\nf(\\mathbf{x})= f(x_1, ..., x_n)\n$$\n* the second order Taylor approx of $f$ at $\\mathbf{a}$ is: \n$$g(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n* analogously to the above, solving for the stationary point of the second order Taylor approx. of $f$ yields:  \n&nbsp;&nbsp;&nbsp;&nbsp;$$\\mathbf{x}^* = \\mathbf{a} - (\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})$$  \nwhere $\\nabla^2 f(\\mathbf{a})$ is the Hessian of $f$ at $\\mathbf{a}$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}^m$*\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n* the second order Taylor approx of the $i^{\\text{th}}$ element of $f$ at $\\mathbf{a}$ is: \n$$g_i(\\mathbf{x}) = f_i(\\mathbf{a}) + \\nabla f_i(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f_i(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n\n*Optimization*:<sup>14</sup>\n\nBased on the above, we can see that to go from a starting point of $\\mathbf{a}$ to the stationary point $\\mathbf{x^*}$ of the quadratic approximation of $f$ at $\\mathbf{a}$, we must move in the descent direction $(\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})$ (for a general $f: \\mathbb{R}^n \\to \\mathbb{R}$).\nThis is how we can iteratively update our estimate of the minimum of $f$:\n\n$$\n\\mathbf{a_{n+1}} = \\mathbf{a_n} - (\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})\n$$\n\n*Root-finding*:\n\nIf we instead seek to find the roots of $f$, rather than setting the derivative of the second order Taylor approx. of $f$ to $0$, we can instead set the linear approx. (first order Taylor approx.) of $f$ to $\\mathbf{0}$:\n$$\n\\begin{aligned}\n\\mathbf{0} &= \\mathbf{f(x)} + \\mathbf{J(x)}(\\mathbf{x^*}-\\mathbf{a})\\\\\n\\mathbf{x^*} &= \\mathbf{a} - \\mathbf{J(x)}^{-1}\\mathbf{f(x)}\n\\end{aligned}\n$$\nIf $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then the Jacobian matrix $J$ is ($m$ x $n$); if $J$ is not invertible then the pseudo-inverse $J^* = (J^TJ)^{-1}J^T$ may be used instead for iteration.\nThe update steps are then:<sup>15</sup>\n$$\n\\mathbf{a_{n+1}} = \\mathbf{a_n} - \\mathbf{J^* f(a_n)}\n$$\nIn the univariate case, Newton's iteration is then:\n$$\na_{n+1} = a_{n} - \\frac{f(a)}{f'(a)}\n$$\n\n##### (Draft) Implementation for optimization\nInputs: $f(\\mathbf{x})$, max steps $m$, initial value $\\mathbf{a}$\nOutputs: $[a_0, a_1, a_2...a_k]$, $[f(a_0), f(a_1), ... f(a_k)]$\n\nPseudocode:<sup>14</sup>\n* get value and gradient (functions) of $f$ from AD implementation\n* get Hessian (function) of $f$\n* $\\text{for } i \\text { in } [1,2, ...m]:$\n    * evaluate value and gradient of $f$ at $a$\n    * evaluate Hessian at $a$\n    * $a = a - (\\nabla^2 f(a))^{-1}\\nabla f(a)$\n\n### Halley’s Method (extension of Newton’s method)\n\nWhereas Newton's method provides quadratic convergence (the number of correct digits 'doubles' every iteration), Halley's method provides cubic convergence\n(the number of correct digits 'triples' every iteration). Halley's iteration is defined as:<sup>19</sup>\n\n$$\nx_{k+1} = x_k - \\frac{2f(x_k)f'(x_k)}{2(f'(x_k))^2 - f(x_k)f''(x_k)}\n$$\n\nOnce methods to take the second derivative are implemented, the implementation of Halley's method should be relatively straightforward given that it involves\nthe evaluation of deriatives at particular values (which can be initialized in AutoDiff objects).\n\n### Solving Linear Algebraic Equations\n\nCurrent popular methods of solving the problems below mostly take an iterative approach, \ne.g. Gauss-Jordan Elimination, Gaussian Elimination with Backsubstitution, LU Decomposition. Solving these problems also take large computation time; we are curious if automatic differentiation will give a better solution.\n\n* Solution of the matrix equation $Ax=b$ for an unknown vector $x$\n* Solution of more than one matrix equation $Ax_j=b_j$, for a set of vectors $x_j$, $j=0,1,..,$ each corresponding to \na different, known right-hand side vector bj\n* Calculation of the matrix $A^{-1}$ that is the matrix inverse of a square matrix $A$, i.e., $A^{-1}\\cdot A=A \\cdot A^{-1} =I$, \nwhere $I$ is the identity matrix   \n* Singular value decomposition of a matrix $A$\n* Linear least-squares problem\n\n    The reduced set of equations to be solved can be written as the $N\\times N$ set of equations\n    \n    $$\n    (A^{T}\\cdot A)\\cdot x = (A^{T}\\cdot b)\n    $$\n\n    where $A^{T}$ denotes the transpose of the matrix A.\n    These quations are called the normal equations of the linear least-squares problem. \n\nUsually, a direct solution of the normal equations is **NOT** generally the best way to find least-squares solutions.\nWe would like to see how automatic differentiation performs in this task and compare the differences. \n","metadata":{"deepnote_cell_type":"markdown","output_cleared":false,"tags":[],"cell_id":"00000-e99b2758-df42-482c-9750-561389e4b6d5"}},{"cell_type":"markdown","source":"<a id='References'></a>\n## IX. References","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00017-5d044ac8-e8ad-49ad-9b77-a930d44af1a3"}},{"cell_type":"markdown","source":"\n\n1. http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/AMLAutoDiff.pdf\n2. http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\n3. \"Automatic Differentiation in Machine Learning: A Survey\" https://www.jmlr.org/papers/volume18/17-468/17-468.pdf\n4. https://harvard-iacs.github.io/2020-CS107/lectures/lecture10/notebook/\n5. https://kenndanielso.github.io/mlrefined/\n6. https://people.cs.umass.edu/~domke/courses/sml/09autodiff_nnets.pdf\n7. https://docs.codecov.io/docs/flags\n8. https://docs.codecov.io/docs/notifications \n9. https://docs.travis-ci.com/user/for-beginners/\n10. https://www.guru99.com/exploratory-testing.html\n11. https://en.wikipedia.org/wiki/Unit_testing#:~:text=Unit%20tests%20are%20typically%20automated,an%20individual%20function%20or%20procedure\n12. https://en.wikipedia.org/wiki/Dual_number#:~:text=Division%20of%20dual%20numbers%20is,cancel%20the%20non%2Dreal%20parts\n13. https://blog.demofox.org/2017/02/20/multivariable-dual-numbers-automatic-differentiation/\n14. https://kenndanielso.github.io/mlrefined/blog_posts/7_Second_order_methods/7_3_Newtons_method.html\n15. http://fourier.eng.hmc.edu/e176/lectures/NM/node21.html\n16. https://www.math.usm.edu/lambers/mat419/lecture9.pdf\n17. http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html\n18. http://web.mit.edu/18.06/www/Spring17/Multidimensional-Newton.pdf\n19. http://cis.poly.edu/~mleung/CS3734/s03/ch05/HalleyIteration.pdf","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00017-045c04bf-f0cc-4f26-90be-627d3c0d838d"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00004-af3e1667-34ae-4b0d-8d7b-c17dfd1a3a46"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00005-b67374f9-3207-4b43-a5f5-a0a8373fe649"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"12c79f6c-e09f-4d53-b92c-2ed3325db852","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}