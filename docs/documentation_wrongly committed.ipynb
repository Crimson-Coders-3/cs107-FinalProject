{"cells":[{"cell_type":"markdown","source":"#THIS IS NOT CORRECT FILE -- USER GUIDE IS CORRECT FILE, THIS WAS COMMITTED INCORRECTLY IN HASTE./n/n/n/n # AutoDiff Documentation\n\n\n## Sections:\n\n[I. Introduction](#Introduction)\n\n[II. Background](#Background)\n\n[III. Software Organization](#Software_Organization)\n\n[IV. How to Use](#How_to_Use)\n\n[V. Implementation](#Implementation)\n\n[VI. Testing](#Testing)\n\n[VII. Implemented Extensions](#Implemented_Extensions)\n\n[VIII. Future Features](#Future_Features)\n\n[IX. References](#References)","metadata":{"tags":[],"cell_id":"00000-588cfcd6-344f-4814-bb60-3d68a4520145","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Installation Guide / Getting Started (TL;DR)\n\nThis is a User Guide to our Auto Differentiation, Root-finding, and Dual Number Libraries.\n\n### 1.1 Installation\n\n1. Clone our GitHub repo:\n```\ngit clone https://github.com/Crimson-Coders-3/cs107-FinalProject/\n```\n\n2. Build third party libraries:\n```\ncd 3PL\n./build_3PL.sh\n```\n3. Execute config.sh from the main project directory: \n```\ncd ..\n./config.sh\n```\n\n### 1.2 Start Coding\n4. To start coding:\n```\ncd App\ncd src\nvim main.cpp\n```\nIn `App/src` you will find `main.cpp` preconfigured for you to call any of the Auto Differentiation, Roots, and Dual libraries.\nYou may notice that `main.cpp` includes only the roots library (`#include \"Roots.h\"`). \nRoots uses ADFunc and ADFuncVector, so including just Roots catches all three in one.\n\nWorking in `App/src`, you should have no problem creating other C++ files and including the header file of the relevant library, you are calling.\nI.e., `#include \"ADFunc.h\", #include \"ADFuncVector.h\", #include \"Dual.h\", #include \"Roots.h\"`.\n\n### 1.3 Compile and Execute\n\n5. To compile your C++ code within `App/src`, edit the `App/src/CMakeLists.txt` file to include any additional `.cpp` files you created.\n```\nnano CMakeLists.txt\n```\n6. Now return the `App` directory and run `./config.sh`\n```\ncd ..\n./configure.sh\n```\n7. To run your executable, enter the `App/bin` directory and run your executable.\n```\ncd bin\n./app_demo\n```\n\n### 1.4 Calling Functions / Function Documentation\n\n8. `App/src/example.cpp` includes some function declarations to get you started.\n```\n#include <math.h>\n#include <string>\n#include <cmath>\n#include \"Roots.h\"\n\nusing namespace std;\n\nvector<ADFunc> customFunct2(Eigen::VectorXd vals){\n     ADFunc x = ADFunc(vals[0], {1,0,0});\n     ADFunc y = ADFunc(vals[1], {0,1,0});\n     ADFunc z = ADFunc(vals[2], {0,0,1});\n     return {5*sin(x), 3*y/2, z+2};\n}\n\n\nint main(){\n    function<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct2;\n    Equation eq(customFunct2, 3);\n    Eigen::VectorXd guess(3);\n    guess << 1, 1, 1;\n    double tol = 1e-4;\n    Eigen::VectorXd roots = eq.getRoots(guess, tol);\n    cout << \"Roots found: \\n\" << roots << endl;\n\n    //Example of getting the Jacobian (as a MatrixXd object)\n    ADFunc x = ADFunc(7, {1,0});\n    ADFunc y = ADFunc(3, {0,1});\n    vector<ADFunc> F2 = {2*pow(x,2), 5*sin(y)*x};\n    Eigen::MatrixXd J = getJ(F2, 2);\n    cout << \"Jacobian of F: \\n\" << J << endl;\n    return 0;\n}\n```\n9. A detailed list of all function declarations and functionality description is in our Doxygen.\nWhen you `git clone`-ed our library, you automatically received our Doxygen documentation.\nFrom the main directory, navigate to `docs/GENERAETED_DOCS/html/index.html`\n```\ncd docs\ncd GENERAETED_DOCS/html\nClick on `docs/GENERAETED_DOCS/html/index.html` and it will launch locally in your browser.\n```\n10. Navigate to `Files/File List`. Click on the file of interest to you, and scroll down to \"Functions.\"\nClick on any of the functions to arrive at the Reference page, where you will find a description of all that library's functions.\n\n### 1.5 Requirements:\n\nHere are some requirements you will need. Also see \"4.3 Possible Error Messages\" in you run into a proble,..\n\n1. cmake:\n\n* tutorial: https://cliutils.gitlab.io/modern-cmake/chapters/intro/installing.html\n* documentation: https://cmake.org/cmake/help/v3.19/\n* useful links: https://github.com/ackirby88/CS107/tree/master/cmake\n\n2. Eigen (linear algebra library)\n\n* overview: http://eigen.tuxfamily.org/dox/index.html\n* source code bundled with our GitHub repo under \"3PL\" (containing the extracted tar.gz file from http://eigen.tuxfamily.org/index.php?title=Main_Page#Download)\n* *following step 2 in the Installation Guide above will build Eigen by default, to the `install` directory*\n\n3. GoogleTest (for testing only, not required)\n\n* additional info.: https://github.com/google/googletest/tree/master/googletest\n* *following step 2 in the Installation Guide above will build GoogleTest to the `install` directory*\n\n4. lcov (for testing only, not required)\n\n* *following step 2 in the Installation Guide above will build lcov by default, to the `install` directory*\n* once the installation completes, you may try \"make test\" to verify \nif it is installed correctly\n* additional info: http://ltp.sourceforge.net/coverage/lcov.php\n* source code repo: https://github.com/linux-test-project/lcov\n\n5. C++ 11\n\n### 1.6 Beyond\n\nIf you need any help using our library or desire to provide feedback (which we are always very eager for!) contact us here:\n- Scarlett Gong: wenlin_gong@g.harvard.edu \n- Morris Reeves: morrisreeves@g.harvard.edu \n- Gayatri Balasubramanian: gayatrib@college.harvard.edu\n\n### 1.7 Broader Impact and Inclusion Statement\n\nSee our mission and principles in section 2 (right below).\n\n## 2 Broader Impact and Inclusion\n\n### 2.1 Broader Impact\nBroader Impact\nFrom Linux to Python and R, open source was one of the most ingenious consequences of the internet. It was a larger-than-life platform to exchange code and come together to build better, more comprehensive software. However, open source has been a platform of something else: an extension of the biases and inequalities in our own society. Perhaps compounded even more by how rapidly open-source code and software develop – like a kid chasing an accelerating train. From spare or difficult documentation to difficult syntax and a lack of transparency in the code, open source software can be impassible. As develops, our foremost intention is to make our software accessible to the world, equally.\nHere, we developed an open-source auto-differentiation library in C++, a field and a programming language both not very accessible to those most affected by the inequalities in computing.  Our strength in making our library transparent and accessible actually came from our weakness. Two of us began this project with no C++ background and one of us had a single year of experience. We experienced firsthand the impassibility of this content – we even watched our teaching staff falter. So, while the process of developing this library was uphill for us, it taught us – made transparent to us – where our users will struggle as well.\nWe acknowledge that software, by definition, has a barrier, because nobody can learn to code nor read code overnight. But we aimed to make an accessible C++ auto-differentiation library through the eyes of our users.\nA user navigates to open source code on GitHub and first navigates to the README.md file. Striving for software inclusivity, we wrote extensive, readable documentation. We organized our documentation as readably as possible. We included a “tl;dr” because often this is the first thing a user looks for. After following documentation to download, a user often next looks for common sense folder names telling them where to begin. Our “Example” folder containing “example.cpp” sets all this up for the user. Next, a user looks for the methods/functions in the library, and our “.h” header files contain extensive comments.\nWe hope that through thinking through the user’s thought process, we have anticipated and understood the diversity of backgrounds they have. We included our contact info, as users who need help with our documentation can reach out to us directly for help. And to those on the more developer side, we included the ability to make pull requests, which we review on a frequent basis.\nWe acknowledge that our software, like all software, can be abused. As a programmer, you must understand that being a good citizen also means being an ethical coder. If you have doubts about usage, reach out to us. While accessible and free of charge, we have the right to notify law enforcement if you use our code for harm.\n\n### 2.2 Inclusivity Statement\nCrimson Coders 3 welcome and encourage you to use and contribute to our auto-differentiation library. Our work is guided by the principles of equality, transparency, respect, giving, and kindness. To us, diversity is not only a multi-identity user community, but the accessibility of our code to the world. We encourage you to contribute if you can or reach out if you need help understanding our library.\n","metadata":{"tags":[],"cell_id":"00001-536edb30-ec21-4e3c-a4e4-ac23f5f4caf5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Introduction'></a>\n## I. Introduction: Our Library and Motivation","metadata":{"tags":[],"cell_id":"00001-b31d80db-41ab-45e7-8edf-b0b23f97e286","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The purpose of our software is to implement a C++ library for Automatic Differentiation through forward mode, Automatic Differentaition through dual numbers, and multi-dimensional Root-Finding.\nOur library provides the efficient function evaluation and derivative evaluation at machine precision for any real-valued or vector-valued function.\n\n### Why Forward Mode and Dual Numbers\nWhile we chose forward mode and dual numbers to implement automatic differentiation, there are two main alternatives to automatic differentiation (\"AD\"): symbolic differentiation and numerical (approximation) methods.\nWhile symbolic differentiation evaluates derivatives to machine precision, it is often costly to evaluate and prone to 'expression swell': \nsymbolic differentiation may result in exponentially large expressions to evaluate. In contrast, numerical methods (such as the finite difference method) are \nquick to evaluate but do not generally result in machine precision: when evaluating $(f(x+\\epsilon)-f(x))/\\epsilon$, use of a large epsilon results in imprecise approximation,\nwhereas use of a small epsilon results in floating point errors. Moreover, the complexity of numerical methods increases as the number of dimensions of the gradient increases.<sup>3</sup>\nAutomatic differentiation circumvents these challenges by \"accumulating\" numerical evaluations of simple elementary functions.\n    \n### Motivation\nThe efficient evaluation of derivatives at machine precision naturally has a wide (and growing) range of application areas, of which a few are highlighted below:\n* Differentiation of functions implemented as code: AD does not require closed-form expressions (as is the case in symbolic differentiation)<sup>3</sup>\n* Backpropagation: training neural networks requires finding weights that minimize the objective function\n    * AD has seen increasingly widespread use since ~2015 as deep learning grows in popularity (e.g. AD implementations in TensorFlow and PyTorch)<sup>3</sup>\n    * Applications include computer vision, NLP\n* Solving ODEs\n* Gradient-based optimization (finding minima through step-wise updates)<sup>3</sup>\n* Root finding, including Newton's method\n\nAD is an important problem not only because it is elegant, but it sits at the heart of many of the interesting questions in computation and real-world problems.","metadata":{"tags":[],"output_cleared":false,"source_hash":"da8d2ffb","execution_start":1605826856856,"execution_millis":5,"cell_id":"00002-b87b60be-fcb9-47fd-bf01-c4c93405eab5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Background'></a>\n## II. Background: Mathematics and Intuition Behind AD","metadata":{"tags":[],"cell_id":"00003-62497fc6-d3f6-47f4-82eb-93be04abafa3","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Suppose we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n\nand suppose that we seek ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$.\n\nIf we have $J_f$, the Jacobian matrix of $f$, we can compute ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ as the product of the Jacobian matrix and the desired seed vector $s$:\n\n$$\nJ_f\\mathbf{s}= \\begin{bmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\[1ex]\n    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\[1ex]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n    \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\[1ex]\n0 \\\\[1ex]\n\\vdots \\\\[1ex]\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} \\\\[1ex]\n\\frac{\\partial f_2}{\\partial x_1} \\\\[1ex]\n\\vdots \\\\[1ex]\n\\frac{\\partial f_m}{\\partial x_1}\n\\end{bmatrix}\n$$\n\nRather than using matrix-vector multiplication or an explicit matrix representation of the Jacobian, the forward mode of AD involves calculating ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ \n(i.e.  a single column of the Jacobian) by computing ${\\frac{\\partial f_i}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ for each of $i=1,...,m$ separately in one or more *forward \"passes\"*\n(also sometimes referred to as \"sweeps\").<sup>2,3</sup> If $f: \\mathbb{R}^1 \\to \\mathbb{R}^m$, only one forward pass would be needed to calculate the full Jacobian, since in that case $n=1$, the Jacobian matrix is $m$x$1$,\nand the seed vector would have a single entry. If $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then $n$ forward passes would be required (one forward pass for each entry in the $n$x$1$ seed vector). \nAlthough the matrix representation of the Jacobian is not explicitly used in the forward mode, this intuition carries over: within each forward pass, the derivative of\none of the variables is set to 1 and the rest to zero (for example, $\\dot{x}_1=1$ if we seek ${\\frac{\\partial f}{\\partial x_1}}$)\n\nAD works by breaking down the function $f$ into a composition of much simpler elemental functions (or elemental operations).\nEach forward pass starts by assigning 1 to the derivative of the variable $x_i$ for which we want to differentiate $f$ with respect to, and assigning 0 to all other variables.\nThen the elemental functions are evaluated from innermost to outermost, accumulating two numerical values at each intermediate variable $v_i$ in the trace<sup>3,4</sup>:\n* the **\"primal\"**: the value of the elemental function at $\\mathbf{x}=\\mathbf{a}$, denoted $v_i$ (i.e. the input of the elemental function will be carried over from any inner functions evaluated earlier in the trace)\n* the **derivative** or \"tangent\", denoted $\\dot{v_i}$ (i.e. the derivative of the elemental function evaluated at $\\mathbf{x}=\\mathbf{a}$)\n\nWithin each step, the derivative $\\dot{v_i}$ is implicitly calculated using the chain rule. For example, if the elemental function corresponding to $v_3$ results from the composition of several functions $w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x}))$ which for clarity we denote $w_3(u_1, u_2)$ then:\n$$\n\\dot{v_3} = {\\frac{\\partial}{\\partial x_1}} w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x})) = {\\frac{\\partial w_3}{\\partial u_1}}{\\frac{\\partial w_2}{\\partial x_1}} + {\\frac{\\partial w_3}{\\partial u_2}}{\\frac{\\partial w_1}{\\partial x_1}}\n$$\nIn forward mode, the innermost elemental function primals and derivatives are evaluated first and 'accumulated' outwards.","metadata":{"tags":[],"cell_id":"00004-6cc87d08-b609-4d04-9379-bcfbfe3516b0","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Software_Organization'></a>\n## III. Software Organization: Directory Structure, Basic Modules, Testing, and [Installation](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md)","metadata":{"tags":[],"cell_id":"00005-0f80b04d-2add-4433-81ee-afae7695680e","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Directory Structure\n\nThe files most relevant to the reader are marked with *s below -- particularly these are `.cpp` and `.h` files. \nThe header files contain function declaratations, meaning that if they user took a look a these files, they would \nhave a good syntatic idea of how to construct and run class methods.\n\nThe folders are organized in this particular way to enable GoogleTest and CMake, and integration with TravisCI and CodeCov.\n**As a user, if you follow the installation guidelines, you will not have to worry about anything except the `App` folder.\nAs a developer, checkout the \"Testing\" section below, as you will have to think about a few more file (the `.sh` files, that is).**\n\nFile structure (* marks files/folder relevant to the user; ^ marks files relevant to developer)\n```\n/cs107-FinalProject\n    + /3PL\n        + googletest-release-1.8.1\n        + lcov\n        | ^build_3PL.sh\n    + /App\n        + /src\n            *CMakeLists.txt\n            **main.cpp\n        + /tests\n        | CmakeLists.txt\n        | *^config.sh\n    + /AutoDiff\n        + /core\n            + /src\n                | *ADFunc.cpp\n                | *ADFuncVector.cpp\n                | CMakeLists.txt\n            + /tests\n        + /include\n            *ADFunc.h\n            *ADFuncVector.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /Dual\n        + /core\n            + /src\n                | *Dual.cpp\n                | CMakeLists.txt\n            + /tests\n            | CMakeLists.txt\n        + /include\n            *Dual.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /Roots\n        + /core\n            + /src\n                | CMakeLists.txt\n                | *Roots.cpp\n            + /tests\n            | CMakeLists.txt \n        + /coverage\n        + /include\n            | *Roots.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /docs\n        + diaries\n        + images\n        | README.md\n        | milestone1.ipynb\n        | milestone2.ipynb\n        | milestone2_progress.ipynb\n        | cocumentation.ipynb\n    | .travis.yml\n    | *README.md\n    | codecov.yml\n    | ^config.sh\n```\n\nWe use GoogleTest to generate local code coverage reports. We use `converge.sh` files to make it easier to call `lcov` and link coverage info to an `index.html` under a generated `coverage` folder. \nTo call `converge.sh`, the user may run `./config.sh -gtest` in the root directory, or run the local coverage report `./coverage.sh`.\n\n### Basic Modules\n\nOur functions per library (AutoDiff, Dual, and Roots) are located in doxygen. We cannot link this for you, as doxygen is run locally on the user's machine offline.\n\nTo view the doxygen documentation, `cd` from the root directory in `docs/GENERAETED_DOCS/html`. Then launch `docs/GENERAETED_DOCS/html/index.html`.\nTo view the doxygen documentation, `cd` from the root directory in `docs/GENERAETED_DOCS/html`. Then launch `docs/GENERAETED_DOCS/html`.\n```\ncd docs/GENERAETED_DOCS/html\nThen click on index.html\n```\nThis will launch locally in your browser.\nNavigate to Files/File List. Click on the file of interest to you, and scroll down to \"Functions.\"\nClick on any of the functions to arrive at the Reference page, where you will find a description of all that library's functions and their specs.\n\n### Testing \nWe testing using GoogleTest. This means that we aim to not only cover every function with our tests, but every line of code as well.\nAs a developer, `cd` into the folder you are working in (`AutoDiff, Dual, Roots`).\n```\ncd AutoDiff\n```\nEnter the `core/tests/src` folder. Here you will see a bunch of `_test_.cpp` files.\n```\ncd core/tests/src\nls\n```\nWrite your own tests in this folder. When you are ready to run, head back to the root `AutoDiff, Dual, Roots`.\nRun `./config.sh` and then `./coverage.sh` (sometimes just running the coverage bash file is not enough to refresh the system that a change has taken place).\n```\ncd ..\ncd ..\ncd ..\n./config.sh\n./coverage.sh\n```\nThanks to GoogleTest, this process has automatically generated an html index page that we can visit and check the coverage and unit tests of our `.cpp` files.\n`cd` into `coverage/CODE_COVERAGE` and launch `index.html` in your local browser by double clicking on it.\n```\ncd coverage/CODE_COVERAGE\nclick on index.html to launch it\n```\nBy looking at the coverage statistics and line by line highlighting, we knew where we had enough unit tests and we needed more.\nWe checked each derivative and evaluation by hand using Wolfram Alpha, allowing up to verify that each function was working as intended.\nThe `core/tests/src` give an excellent illustration of how to complete tests, and we refer the user to these if they intend to do development with our work.\n\n### Installation\n\nWe refer you to our [installation guide](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md).","metadata":{"tags":[],"cell_id":"00006-1085f260-4d52-4f15-8c8d-ac0cac4969e6","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='How_to_Use'></a>\n## IV. How to Use","metadata":{"tags":[],"cell_id":"00007-b6491b2c-81e1-4e45-90e4-f87cded0f497","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Configuration\n\nWe again refer you to our [installation guide](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md).\n\nAs a user you just need to follow those instruction and work in `App/src`. The files relevant to you are *-ed in the \"Software Organization\" section, if you need a visual cue.\nAs a developer, refer to the \"Software Organization\" section for a visual layout of which files matter to you, and for a description of how to run tests.\n\nRemember, when you are developing and add functionality or new files, make sure that the `coverage.sh` and `config.sh` files are tracking your additions.\nMake sure you write comprehensive test in the `core/tests/src` folder as detailed in the \"Testing\" section above.\n\n**Note: If you get a \"permission denied\", you need to check if config.sh, coverage.sh, or Makefile is executable. Type \"chmod +x \\[executable file\\]\" in the command prompt to add files to be executable.**\n\nOther common problems in the \n\n### Operating system: \n\n1. MacOS\n\nCurrent \"Dual/CMakeLists.txt\" and \"AutoDiff/CMakeLists.txt\" comment out two pieces that make it different from\nLinux version. \n\nThe 1st one is \"line 10: set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib)\". If you receive an error like \"realpath: command not found\", you should consider line10 out.\n\nThe 2nd one is \"line 27: if(CMAKE_C_COMPILER_ID MATCHES \"GNU\")\" and correspondingly \"line 30: endif()\". It maybe due to that MacOS use LLVM gcov to emulate gcov, if MacOS users don't commit these two lines out, the unittesting won't be triggered. Therefore, a code coverage report website cannot be generated.\n\n2. Linux\n\nUncomment the two parts (line10, line27, line30) and it should work. ","metadata":{"tags":[],"cell_id":"00008-681908cf-498d-4909-a001-0b6e5e6b13a7","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Usage Examples (Setup)\n\n``git clone https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/m2b-dev``\n\n``cd 3PL``\n\n``./build_3PL.sh``\n\n``cd ..``\n\n``./config.sh ``\n\nif you want to get test coverage, \n\n``./config.sh -gtest``\n\n\n### Usage Examples\nPlease note that currently nested functions are not supported and each step in the trace has to be initialized separately. For example:\n\nFor instance this won’t work:\n```\nAutoDiff test = AutoDiff(4.0);\nAutoDiff test1 = sin(pow(test, 3.0));\n```\n\nBut this works:\n```\nAutoDiff test = AutoDiff(4.0);\nAutoDiff test1 = pow(test, 3.0):\nAutoDiff test2 = sin(test1);\n```\n\nTo get the function evaluation, call `test2.val` or `test2.getVal()`. To get the function derivative by forward mode, call `test2.der` or `test2.getDer()`.\n\nPrint out the evaluation and derivative using `test2.print()`.\n\n\n### External Dependencies\n\nThis project uses the Prof Kirby's skeleton project and `Math.h`.","metadata":{"tags":[],"cell_id":"00009-1a9b505a-8340-46ec-a2d9-21114348d100","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Implementation'></a>\n## V. Implementation","metadata":{"tags":[],"cell_id":"00010-6caeb89e-6ac0-4c50-bbfd-049a54df8742","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"1 Operator Overload\n\nOperator overloading the key of success for our Automatic Differentiation application. It enables us calculate\nthe current value of function and partial derivatives with respect to all the variables at the same time. \nInternally, there is a vector of double structures that stores all the partial derivatives. And each time when\na ADFunc object get operated, it will loop through all the currently partial derivatives vector and update\neach of them using derivative rules. \n\n'''\n#include \"ADFunc.h\"\n#include \"ADFuncVector.h\"\n#include <math.h>\n#include <typeinfo>\n#include <iostream>\n#include <string>\n#include <cmath>\n\nusing namespace std;\n\n    // Example 1: x is a single number, f(x) is a function with a single variable\n    // f.dval is a single number representing df/dx\n    ADFunc x(5.0, vector<double> {1.0});\n    ADFunc f = pow(x,2) + 5*exp(x);\n    cout << \"df/dx: \" << f.dval_wrt(0) << endl; //752.07\n    cout << \"f value: \" << f.val() << \"\\n\" << endl; //767.07\n'''\n2 multi_vars()\n\nCreate multiple functions/variables and intilaize seed vectors as unit vectors\nautomatically for you by doing a for-loop internally. \n\n'''\n    // Example 2: (x1,...,xn) vector inputs with a single function f(x1,..., xn)\n    // f.dval is a vector with two elements, [df/dx1, df/d2, ...]\n    vector<double> init_values = {5.0, 3.0};\n    vector<ADFunc> multi_vars = multiVar(init_values);\n    x = multi_vars[0];\n    ADFunc y = multi_vars[1];\n    f = pow((y*x), 2) + 7*sin(log(x));\n\n    vector<double> dvals = f.dval_wrt(vector<int> {0,1});\n    cout << \"df/dx: \" << dvals[0] << endl; //89.9459\n    cout << \"df/dy: \" << dvals[1] << endl; //150\n    cout << \"f2 val: \" << f.val() << \"\\n\" << endl; //231.995\n'''\n\n3 ADFuncVector supports many dval_wrt() that makes getting partial derivative easier and save your time of doing for-loop.\nIt is like a wrapper class of ADFunc.\n\n''' \n    // Example 3: x is a single number with a vectorized function F(x)\n    // F(x) = [f1(x), f2(x), ..., fm(x)], each f_i(x) is a function with a single variable\n    // F.dval is s a vector size m*1 [df1/dx, df2/dx, ..., dfm/dx]\n    vector<ADFunc> F = {2*pow(x,2), 5*sin(x)*x};\n    ADFuncVector Fvec(F);\n    std::vector<std::pair<int, int> > fun_var_indexs = { {0,0}, {1,0} };\n    \n    dvals = Fvec.dval_wrt(fun_var_indexs);\n    cout << \"df1/dx: \" << dvals[0] << endl; // 20.0\n    cout << \"df2/dx: \" << dvals[1] << endl; // 2.29693\n    cout << \"val for f1: \" << Fvec.val(0) << endl; // 50.0\n    cout << \"val for f2: \" << Fvec.val(1) << \"\\n\" <<endl; // -23.9731\n'''\n'''\n\n    // Example 4: x is a vector [x1, x2, ..., xn], \n    // F(x) is a vector of functions [f1(x), f2(x), ..., fm(x)]\n    // f.der is a matrix with self defined size\n    init_values.push_back(0.3);\n    multi_vars = multiVar(init_values);\n    x = multi_vars[0];\n    y = multi_vars[1];\n    ADFunc z = multi_vars[2];\n    F = {2*pow(x,z)/y, 5*sin(y)*x+z/sin(y)*sinh(y), 9*sinh(z)-exp(x*y)};\n    Fvec = ADFuncVector(F);\n    vector<vector<std::pair<int, int> > > fun_var_index_m = { {{0,0},{2,1},{0,2}},{{1,0},{1,1},{2,2}}};\n\n    vector<vector<double> > dval_m = Fvec.dval_wrt(fun_var_index_m);\n    cout << \"df1/dx1: \" << dval_m[0][0] << endl; // 0.0648263\n    cout << \"df1/dx2: \" << dval_m[0][1] << endl; // -1.63450869e+07\n    cout << \"df1/dx3: \" << dval_m[0][2] << endl; // 1.7389\n    cout << \"df2/dx1: \" << dval_m[1][0] << endl; // 0.7056\n    cout << \"df2/dx2: \" << dval_m[1][1] << endl; // 146.053\n    cout << \"df2/dx3: \" << dval_m[1][2] << endl; // 9.40805\n    cout << \"val for f1 in F: \" << Fvec.val(0) << endl; // 1.08\n    cout << \"val for f2 in F: \" << Fvec.val(1) << endl; // 24.8245\n    cout << \"val for f3 in F: \" << Fvec.val(2) << endl; // -3269014.63\n'''","metadata":{"tags":[],"cell_id":"00011-a5bc591e-bdd1-4d7f-8867-f9ddd680b9fb","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Testing'></a>\n## VI. Testing","metadata":{"tags":[],"cell_id":"00012-3ff19143-908b-4bea-9438-1fee227e96f7","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"* Location：Dual/code/tests and AutoDiff/code/tests\n\n    * We have pre-defined several unit tests under the above directories\n    * To run these tests, please (1) run the AutoDiff/code/config.sh file and (2) run the AutoDiff/code/coverage.sh file\n\n* Tools:\n\n    * CodeCov: \n    \n        CodeCov provides statistics of code coverage so that each commit and pull request is measured and in control. \n        To do this, we need to create a .yaml file and specify our metrics (target at least 90%). \n        As our software covers two parts of functions: the basic function(AD implementation) and extensions, \n        it is reasonable to use tags in the .yaml file to separate coverage reports for different modules<sup>7</sup>. \n        Setting Codecov to send notifications to the Slack channel is also a good idea<sup>8</sup>, so everyone in the group \n        gets notified when there is an update.\n    \n    * TravisCI: \n        \n        TravisCI is a continuous integration platform that automatically builds and tests code changes. \n        Instead of merging large pieces of code and integrate them all in the end, we want to take a continuous \n        integration approach - merge in small code changes and get immediate feedback when each commit and pull \n        request happens<sup>9</sup>. We also need a .yaml file to use it. It can also send notifications through Slack. \n* Plan: \n    * Step 1: Exploratory Testing\n\n        Exploratory testing is a simultaneous process of test design and test execution all done at the same time, \n        contrarily to scripted testing, where all test cases are determined in advance<sup>10</sup>.\n        We will note down ideas about what to test and how it can be tested in a document file (.md). \n        We will update this document with additional test case designs, critically evaluate current test results with \n        respect to our expectations, and learn from the testing constantly.\n\n    * Step 2: Unit tests\n\n        Unit tests isolate issues that may arise and each unit is tested independently. \n        We will define the smallest unit to a each Class method. \n        By writing tests for the smallest testable units, then the compound behaviors between those, \n        we can build up comprehensive tests for more complex applications.<sup>11</sup>\n\n    * Step 3: Acceptance tests\n\n        After unit testings, we want to validate software against functional requirements/specifications. We mainly concentrate on:\n\n        * Functionality\n\n        * Basic Usability: Can users freely navigate through the screens without any difficulties?\n\n        * Error Conditions: Do suitable error messages display? Are they clear and proper?\n\n        * Accessibility\n\n\n        \n\n* Evaluations:\n\n    * Code coverage\n\n    * Risk analysis: What are the potential risks of current implementation? What are the important ones and what need to be addressed? \n\n    * Test execution log: recordings on the test execution","metadata":{"tags":[],"cell_id":"00013-2d1b85d4-3233-4c37-9516-76a3ecf6dc97","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Implemented_Extensions'></a>\n## VII. Implemented Extensions","metadata":{"tags":[],"cell_id":"00014-f9285845-6e97-40b1-969b-9255079e6383","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Dual Numbers\n\nDual number is an interesting type of number that uses the symbol $\\epsilon$ (epsilon) and offers how to do automatic\ndifferentiation in another persepective. It automatically calculates the derivative and the value of a function at the same time! \n\nDual numbers are pretty similar to imaginary numbers. A dual number has a real part and a dual part. We write\n\n$$\nx = a + b\\epsilon\n$$\n\nBut! For imaginary numbers, $i^2 = -1$, whereas for dual numbers, $\\epsilon^2 = 0$ (and $\\epsilon$ is not 0!). \n\n#### Basic Math\n\nAdd and subtraction of dual numbers are the same as adding complex numbers: we just add the real and dual parts separately.\n\n$$\n(3+6\\epsilon) + (1+2\\epsilon) = (3+1)+(6\\epsilon+2\\epsilon) = 4+8\\epsilon\n$$\n\n$$\n(3+6\\epsilon) - (1+2\\epsilon) = (3-1)+(6\\epsilon-2\\epsilon) = 2+4\\epsilon\n$$\n\nTo multiply dual numbers, we use F.O.I.L. just like we do with complex numbers\n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 6\\epsilon +6\\epsilon + 12\\epsilon^2 = 3 + 12\\epsilon + 12\\epsilon^2\n$$\n\nAs $\\epsilon^2=0$, the last item $12\\epsilon^2$ disappears. \n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 12\\epsilon\n$$\n\nThe division process relates to the conjugate, like complex division - multiplying the denominator  by its conjugate \nin order to cancel the non-real parts.<sup>12</sup> The formula is given below:\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= \\frac{(a+b\\epsilon)(c-d\\epsilon)}{(c+d\\epsilon)(c-d\\epsilon)}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2 - d^2\\epsilon^2}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2}\\\\\n&= \\frac{ac+(bc-ad)\\epsilon}{c^2}\\\\\n&= \\frac{a}{c}+\\frac{(bc-ad)\\epsilon}{c^2}\n\\end{aligned}\n$$\n\nwhich is defined when $c$ is non-zero. If, on the other hand, $c$ is zero while $d$ is not, then the equation\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= k\\\\\n\\frac{a+b\\epsilon}{d\\epsilon} &= k\\\\\na+b\\epsilon &= kd\\epsilon\\\\\n\\end{aligned}\n$$\n\n1. has no solution if $a$ is nonzero\n2. otherwise is solved by any dual number of the form $k=\\frac{b}{d}+y\\epsilon$. To expalins this, suppose we have $k=x+y\\epsilon$: \n\n$$\n\\begin{aligned}\nb\\epsilon &= kd\\epsilon\\\\\nb\\epsilon &= (x+\\epsilon y)d\\epsilon\\\\\nb\\epsilon &= xd\\epsilon + \\epsilon^2 y\\\\\nb\\epsilon &= xd\\epsilon\\\\\nx &= \\frac{b}{d}\\epsilon\n\\end{aligned}\n$$\n\nSo, $k=\\frac{b}{d}+y\\epsilon$ and is the value of $\\frac{a+b\\epsilon}{c+d\\epsilon}$.\n\n#### Using $\\epsilon^2$ for AD\n\nSuppose we have a function: $\nf(x) = 3x + 2$, and calculates $f(4)$ and $f'(4)$. \n\nWe know $4 = 4 + 1\\epsilon$, using $1$ for the dual component, since $x$ has a derivative of $1$.<sup>13</sup> We take this into the function:\n\n$$\nf(4) = f(4 + \\epsilon) = (4 + 1\\epsilon) \\times 3 + 2 + 1\\epsilon = (12 + 3\\epsilon) + 2 = 14 + 3\\epsilon\n$$\n\nWe can find out the real number component $(14)$ is the value of $f(4)$ and the dual component $(3)$ is the derivative $f'(4)$, \nwhich is verified below:\n\n$$\nf(x) = 3x+2 = 3\\times4 + 2 = 14\n$$\n\n$$\nf'(x) = 3\n$$\n\nIt is not an coincidence! Now try a quadratic example: $f(x)=5x^2 + 3x + 1$, also evaluates at $f(4)$.\n\n$$\n\\begin{aligned}\nf(4 + \\epsilon) &=5(4 + \\epsilon)^2 + 3\\times(4 + \\epsilon) + 1 \\\\\n &= 5(16 + 8\\epsilon+\\epsilon^2 ) + 12 + 3\\epsilon + 1\\\\\n &= 80 + 40\\epsilon + 5\\epsilon^2 + 12 + 3\\epsilon + 1\\\\\n &= (80+12+1) + (40+3)\\epsilon + 5\\epsilon^2 \\\\\n &= 93 + 43\\epsilon + 5\\epsilon^2\\\\\n &= 93 + 43\\epsilon\n\\end{aligned}\n$$\n\nNow verify it:\n\n$$\n\\begin{aligned}\nf(4)&=5\\times 4^2 + 3\\times 4 +1\\\\ &= 5\\times 16 + 12 + 1\\\\ &= 80 + 12 + 1 \\\\&= 93\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\nf'(x)&=10x + 3\\\\ \nf'(4)&=10\\times 4 + 3 = 43\n\\end{aligned}\n$$\n\nWe can also use dual number on trigonometric functions to do differentiation. \nWhat's more, it is not only limited to the 1st derivative.\nIt is possible to modify this technique to get the 2nd the 3rd, or the Nth derivative of a function. But in a 2nd derivative\nscenario, we need to define $\\epsilon^3=0$, instead of $\\epsilon^2=0$, so on and so forth.\nWe can also extend this to multivariable functions. To do this, we need to define $\\epsilon_x$, $\\epsilon_y$, ... for each variables.\n\nTo conclude, using dual numbers gives us exact derivatives, within the limitations of floating point math \n(compared to finite difference method). \n\n#### Dual Number Implementation\n\nIt is tempting to create a \"DualNumber\" Class and overloads different operators$(+,-,*,/)$ and math functions, eg. sqrt, pow, sin, cos, tan, atan, ... \nWe keep all independent variables in a array-like container and in each overloading function, we loop through this array to calcualte derivative for each variables.\n\nJust like what stated above, dual number can be expressed as the followings:\n\n$$\nz = a+b\\epsilon\n$$\n$$\n{\\epsilon}^{2} = 0\n$$\n\nFor the purpose of auto differentiation, we can use dual numbers in this way:\n\n$$\nz = x + x^{'}\\epsilon\n$$\n\nNext, we can get a dual number cheat sheet:\n\n<img src=\"images/dual_number_table.png\" width=\"800\">\n\nSo, we can plan to make a Dual class and use a type name T to let it can be used as float, double ... etc.","metadata":{"tags":[],"cell_id":"00015-3bb14e85-c8ea-4090-abed-2cdae5d36750","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='Future Features'></a>\n## VIII. Future Features ","metadata":{"tags":[],"cell_id":"00016-639ed963-10cf-4454-813b-df78773fd555","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Root Finding and Optimization\n\n#### Newton's method (also known as the Newton-Raphson Method):\n\n##### General properties and overview\n* \"second order method\" in that the quadratic approximation for $f$ at a local point $a$ is used to compute the descent direction\n(direction in which to step to reach stationary points)\n* well-suited for optimizing or finding roots of convex functions with a moderate number of inputs (e.g. linear regression, logistic regression)\n* requires fewer steps to converge than methods which use linear (rather than quadratic) approximations of the function\n* with non-convex functions, the algorithm may converge to local maxima\n* as with other local optimization methods, starts from a single starting point and iteratively \"refines\" it by stepping in a descent direction\n* for optimization, the idea is to start at point $\\mathbf{a_i}$ and go to $\\mathbf{a_{i+1}}$ by creating second order Taylor series approx. to $f$ at $a_i$ and traveling to the stationary point of this second order Tayor series approx.<sup>14</sup>\n\n##### Theory\nBuilding from simple to more general cases of the functional form of $f$ provides intuition for Newton's method:<sup>15, 16, 17</sup>\n\n*$f: \\mathbb{R} \\to \\mathbb{R}$*\n$$\nf(x) = f(x)\n$$\n* the second order Taylor approx. of $f$ at $\\mathbf{a}$ is: \n$$g(x) = f(a) + \\frac{d}{dx} f(a) (x-a)+\\frac{1}{2}\\frac{d^2}{dx^2}f(a)(x-a)^2$$\n* we can solve for the stationary point of the second order Taylor approx. of $f$ by setting its derivative to 0:\n$$\n\\begin{aligned}\n\\frac{d}{dx}g = \\frac{d}{dx}f(a) + \\frac{1}{2}\\frac{d^2}{dx^2}f(a)(2x^{*} - 2a) &= 0\\\\\n\\frac{d^2}{dx^2}f(a)(x^{*} - a) &= -\\frac{d}{dx}f(a)\\\\\nx^* &= a - \\frac{\\frac{d}{dx}f(a)}{\\frac{d^2}{dx^2}f(a)}\\\\\n\\end{aligned}\n$$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}$*\n$$\nf(\\mathbf{x})= f(x_1, ..., x_n)\n$$\n* the second order Taylor approx of $f$ at $\\mathbf{a}$ is: \n$$g(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n* analogously to the above, solving for the stationary point of the second order Taylor approx. of $f$ yields:  \n&nbsp;&nbsp;&nbsp;&nbsp;$$\\mathbf{x}^* = \\mathbf{a} - (\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})$$  \nwhere $\\nabla^2 f(\\mathbf{a})$ is the Hessian of $f$ at $\\mathbf{a}$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}^m$*\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n* the second order Taylor approx of the $i^{\\text{th}}$ element of $f$ at $\\mathbf{a}$ is: \n$$g_i(\\mathbf{x}) = f_i(\\mathbf{a}) + \\nabla f_i(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f_i(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n\n*Optimization*:<sup>14</sup>\n\nBased on the above, we can see that to go from a starting point of $\\mathbf{a}$ to the stationary point $\\mathbf{x^*}$ of the quadratic approximation of $f$ at $\\mathbf{a}$, we must move in the descent direction $(\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})$ (for a general $f: \\mathbb{R}^n \\to \\mathbb{R}$).\nThis is how we can iteratively update our estimate of the minimum of $f$:\n\n$$\n\\mathbf{a_{n+1}} = \\mathbf{a_n} - (\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})\n$$\n\n*Root-finding*:\n\nIf we instead seek to find the roots of $f$, rather than setting the derivative of the second order Taylor approx. of $f$ to $0$, we can instead set the linear approx. (first order Taylor approx.) of $f$ to $\\mathbf{0}$:\n$$\n\\begin{aligned}\n\\mathbf{0} &= \\mathbf{f(x)} + \\mathbf{J(x)}(\\mathbf{x^*}-\\mathbf{a})\\\\\n\\mathbf{x^*} &= \\mathbf{a} - \\mathbf{J(x)}^{-1}\\mathbf{f(x)}\n\\end{aligned}\n$$\nIf $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then the Jacobian matrix $J$ is ($m$ x $n$); if $J$ is not invertible then the pseudo-inverse $J^* = (J^TJ)^{-1}J^T$ may be used instead for iteration.\nThe update steps are then:<sup>15</sup>\n$$\n\\mathbf{a_{n+1}} = \\mathbf{a_n} - \\mathbf{J^* f(a_n)}\n$$\nIn the univariate case, Newton's iteration is then:\n$$\na_{n+1} = a_{n} - \\frac{f(a)}{f'(a)}\n$$\n\n##### (Draft) Implementation for optimization\nInputs: $f(\\mathbf{x})$, max steps $m$, initial value $\\mathbf{a}$\nOutputs: $[a_0, a_1, a_2...a_k]$, $[f(a_0), f(a_1), ... f(a_k)]$\n\nPseudocode:<sup>14</sup>\n* get value and gradient (functions) of $f$ from AD implementation\n* get Hessian (function) of $f$\n* $\\text{for } i \\text { in } [1,2, ...m]:$\n    * evaluate value and gradient of $f$ at $a$\n    * evaluate Hessian at $a$\n    * $a = a - (\\nabla^2 f(a))^{-1}\\nabla f(a)$\n\n### Halley’s Method (extension of Newton’s method)\n\nWhereas Newton's method provides quadratic convergence (the number of correct digits 'doubles' every iteration), Halley's method provides cubic convergence\n(the number of correct digits 'triples' every iteration). Halley's iteration is defined as:<sup>19</sup>\n\n$$\nx_{k+1} = x_k - \\frac{2f(x_k)f'(x_k)}{2(f'(x_k))^2 - f(x_k)f''(x_k)}\n$$\n\nOnce methods to take the second derivative are implemented, the implementation of Halley's method should be relatively straightforward given that it involves\nthe evaluation of deriatives at particular values (which can be initialized in AutoDiff objects).\n\n### Solving Linear Algebraic Equations\n\nCurrent popular methods of solving the problems below mostly take an iterative approach, \ne.g. Gauss-Jordan Elimination, Gaussian Elimination with Backsubstitution, LU Decomposition. Solving these problems also take large computation time; we are curious if automatic differentiation will give a better solution.\n\n* Solution of the matrix equation $Ax=b$ for an unknown vector $x$\n* Solution of more than one matrix equation $Ax_j=b_j$, for a set of vectors $x_j$, $j=0,1,..,$ each corresponding to \na different, known right-hand side vector bj\n* Calculation of the matrix $A^{-1}$ that is the matrix inverse of a square matrix $A$, i.e., $A^{-1}\\cdot A=A \\cdot A^{-1} =I$, \nwhere $I$ is the identity matrix   \n* Singular value decomposition of a matrix $A$\n* Linear least-squares problem\n\n    The reduced set of equations to be solved can be written as the $N\\times N$ set of equations\n    \n    $$\n    (A^{T}\\cdot A)\\cdot x = (A^{T}\\cdot b)\n    $$\n\n    where $A^{T}$ denotes the transpose of the matrix A.\n    These quations are called the normal equations of the linear least-squares problem. \n\nUsually, a direct solution of the normal equations is **NOT** generally the best way to find least-squares solutions.\nWe would like to see how automatic differentiation performs in this task and compare the differences. \n\n\n# Possible Error Messages\n\n**1. Cannot merge previous GCDA file: corrupt arc tag (0x00000000)**\n\nIf you see this error when you want to see code coverage, it probably relates to the previously\ngenerated coverage, build, or install folder being used to generate code coverage but they are not \ncompatible with your current code. To solve this, you can delete these folders and rerun `./config.sh` \nand then `./coverage.sh`. \n\n**2. Permission denied **\n\nThis error relates to permission denied to access or execute a file. \nFirst, you would need to check if the file being executed exist in the directory. \nSecond, you may use `chmod +x filename` to make the file executable\nThird, you can try adding sudo in front of the command.\n\n**3. '\\r': command not found**\n\nThis is a possible error on a WSL like Ubuntu for Windows. \nTo solve this, first get `dos2unixsudo` using `sudo apt-get install dos2unix`.\nThen for each of the seven .sh files (config.sh, coverage.sh, or build_3PL.sh), run `dos2unix config.sh`, `dos2unix coverage.sh`, and `dos2unix build_3PL.sh`.\nReturn to the final project directory and run `./config.sh`.\n\n**4. cmake: command not found**\n\nYou need to install CMake package.\n\nIn Windows Ubuntu: `sudo apt-get install cmake`\n\nIf you have brew in Mac, `brew install cmake`\n\nIn Linux: see this useful link \n\nhttps://geeksww.com/tutorials/operating_systems/linux/installation/downloading_compiling_and_installing_cmake_on_linux.php\n\n**5. lcov: command not found**\n\nYou need to install lcov packge. We have the package already downloaded for you!\nTo fix this, enter the `3PL/lcov` folder and \n`make install` (or `sudo make install` if you get an \"permission denied\" error).\nIf you have `'\\r': command not found` errors, you will have `dos2unix` several `.sh` files in the \n`3PL/lcov/bin` directory.\n\n## Broader Impact and Inclusivity Statement\n\n### 2.1 Broader Impact\nFrom Linux to Python and R, open source was one of the most ingenious consequences of the internet. It was a larger-than-life platform to exchange code and come together to build better, more comprehensive software. However, open source has been a platform of something else: an extension of the biases and inequalities in our own society. Perhaps compounded even more by how rapidly open-source code and software develop – like a kid chasing an accelerating train. From spare or difficult documentation to difficult syntax and a lack of transparency in the code, open source software can be impassible. As develops, our foremost intention is to make our software accessible to the world, equally.\nHere, we developed an open-source auto-differentiation library in C++, a field and a programming language both not very accessible to those most affected by the inequalities in computing.  Our strength in making our library transparent and accessible actually came from our weakness. Two of us began this project with no C++ background and one of us had a single year of experience. We experienced firsthand the impassibility of this content – we even watched our teaching staff falter. So, while the process of developing this library was uphill for us, it taught us – made transparent to us – where our users will struggle as well.\nWe acknowledge that software, by definition, has a barrier, because nobody can learn to code nor read code overnight. But we aimed to make an accessible C++ auto-differentiation library through the eyes of our users.\nA user navigates to open source code on GitHub and first navigates to the README.md file. Striving for software inclusivity, we wrote extensive, readable documentation. We organized our documentation as readably as possible. We included a “tl;dr” because often this is the first thing a user looks for. After following documentation to download, a user often next looks for common sense folder names telling them where to begin. Our “Example” folder containing “example.cpp” sets all this up for the user. Next, a user looks for the methods/functions in the library, and our “.h” header files contain extensive comments.\nWe hope that through thinking through the user’s thought process, we have anticipated and understood the diversity of backgrounds they have. We included our contact info, as users who need help with our documentation can reach out to us directly for help. And to those on the more developer side, we included the ability to make pull requests, which we review on a frequent basis.\nWe acknowledge that our software, like all software, can be abused. As a programmer, you must understand that being a good citizen also means being an ethical coder. If you have doubts about usage, reach out to us. While accessible and free of charge, we have the right to notify law enforcement if you use our code for harm.\n\n### 2.2 Inclusivity Statement\nCrimson Coders 3 welcome and encourage you to use and contribute to our auto-differentiation library. Our work is guided by the principles of equality, transparency, respect, giving, and kindness. To us, diversity is not only a multi-identity user community, but the accessibility of our code to the world. We encourage you to contribute if you can or reach out if you need help understanding our library.\n\n## Contact Us/ Pull Requests\n\nIf you need any help using our library or desire to provide feedback (which we are always very eager for!) contact us here:\n- Scarlett Gong: wenlin_gong@g.harvard.edu \n- Morris Reeves: morrisreeves@g.harvard.edu \n- Gayatri Balasubramanian: gayatrib@college.harvard.edu\n\n## Future\n\nWe were most inspired by the questions of what motivates AD, and how to make our work more accessible to peopleof all backgrounds, since that is at the core our experience as amateur coders.\n\n- On the interactability side, we were inspired by the Heroku App, because that took AD to an audience beyond coders. Not only that, but it made the experience of working with a programming language math library so much more enjoyable. We want to add some sort of interactive, trasnparent interface that takes our library beyond the scope of those who work in C++.\n\n- Second, while root finding looks for where a function crosses the x-axis, we wanted to take that concept and appy it to the first defivative of a function, thereby giving us the local min and max ofthe function. We always wanted to expant that to the the second derivative test, which would tell us concavity and the second-derivative test. We were talking about AD's rise with deep learning, neural networks, and gradient descent. Core to that is the ability to to apply the first and second derivative test to solve optimization problems.\n\n- Lastly, we spent time in this course working with plotting. Another way to make our application more accessible would be to have a graphical way to plot a function and the tangent line at a point. For this, we have everything we need in terms of evaluating the derivative at a point. All we would need is to have a scale (like in Desmos for example), and to plot the result of our derivative and evaluation at a point.","metadata":{"output_cleared":false,"tags":[],"cell_id":"00017-53e5dff6-6069-45a9-8589-2b8d260269af","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a id='References'></a>\n## IX. References","metadata":{"tags":[],"cell_id":"00018-4a5db13c-e179-4e69-943f-8ff8448462a3","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n\n1. http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/AMLAutoDiff.pdf\n2. http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\n3. \"Automatic Differentiation in Machine Learning: A Survey\" https://www.jmlr.org/papers/volume18/17-468/17-468.pdf\n4. https://harvard-iacs.github.io/2020-CS107/lectures/lecture10/notebook/\n5. https://kenndanielso.github.io/mlrefined/\n6. https://people.cs.umass.edu/~domke/courses/sml/09autodiff_nnets.pdf\n7. https://docs.codecov.io/docs/flags\n8. https://docs.codecov.io/docs/notifications \n9. https://docs.travis-ci.com/user/for-beginners/\n10. https://www.guru99.com/exploratory-testing.html\n11. https://en.wikipedia.org/wiki/Unit_testing#:~:text=Unit%20tests%20are%20typically%20automated,an%20individual%20function%20or%20procedure\n12. https://en.wikipedia.org/wiki/Dual_number#:~:text=Division%20of%20dual%20numbers%20is,cancel%20the%20non%2Dreal%20parts\n13. https://blog.demofox.org/2017/02/20/multivariable-dual-numbers-automatic-differentiation/\n14. https://kenndanielso.github.io/mlrefined/blog_posts/7_Second_order_methods/7_3_Newtons_method.html\n15. http://fourier.eng.hmc.edu/e176/lectures/NM/node21.html\n16. https://www.math.usm.edu/lambers/mat419/lecture9.pdf\n17. http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html\n18. http://web.mit.edu/18.06/www/Spring17/Multidimensional-Newton.pdf\n19. http://cis.poly.edu/~mleung/CS3734/s03/ch05/HalleyIteration.pdf","metadata":{"tags":[],"cell_id":"00019-7216df38-7762-427c-bf0e-1e73694f2613","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"output_cleared":false,"cell_id":"00020-57083b33-bb47-4da2-90e4-299dda3e7a77","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"output_cleared":false,"cell_id":"00021-bb18153c-e3d5-4ca9-ada0-72a1e70b265b","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"8508ce21-872f-46dd-99d9-1cb2e4357782","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}
