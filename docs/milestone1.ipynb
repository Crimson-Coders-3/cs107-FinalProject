{"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThe purpose of our software and of automatic differentiation is to efficiently evaluate derivatives at machine precision.\nThere are two main alternatives to automatic differentiation (\"AD\") for the evaluation of derivatives: symbolic differentiation and numerical (approximation) methods.\nWhile symbolic differentiation evaluates derivatives to machine precision, it is often costly to evaluate and prone to 'expression swell': \nsymbolic differentiation may result in exponentially large expressions to evaluate. In contrast, numerical methods (such as the finite difference method) are \nquick to evaluate but do not generally result in machine precision: when evaluating $(f(x+\\epsilon)-f(x))/\\epsilon$, use of a large epsilon results in imprecise approximation,\nwhereas use of a small epsilon results in floating point errors. Moreover, the complexity of numerical methods increases as the number of dimensions of the gradient increases.<sup>3</sup>\nAutomatic differentiation circumvents these challenges by \"accumulating\" numerical evaluations of simple elementary functions.\n    \nThe efficient evaluation of derivatives at machine precision naturally has a wide (and growing) range of application areas, of which a few are highlighted below:\n* Differentiation of functions implemented as code: AD does not require closed-form expressions (as is the case in symbolic differentiation)<sup>3</sup>\n* Backpropagation: training neural networks requires finding weights that minimize the objective function\n    * AD has seen increasingly widespread use since ~2015 as deep learning grows in popularity (e.g. AD implementations in TensorFlow and PyTorch)<sup>3</sup>\n    * Applications include computer vision, NLP\n* Solving ODEs\n* Gradient-based optimization (finding minima through step-wise updates)<sup>3</sup>\n* Root finding, including Newton's method\n\n## Background\n\nSuppose we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n\nand suppose that we seek ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$.\n\nIf we have $J_f$, the Jacobian matrix of $f$, we can compute ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ as the product of the Jacobian matrix and the desired seed vector $s$:\n\n$$\nJ_f\\mathbf{s}= \\begin{bmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\[1ex]\n    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\[1ex]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n    \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\[1ex]\n0 \\\\[1ex]\n\\vdots \\\\[1ex]\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} \\\\[1ex]\n\\frac{\\partial f_2}{\\partial x_1} \\\\[1ex]\n\\vdots \\\\[1ex]\n\\frac{\\partial f_m}{\\partial x_1}\n\\end{bmatrix}\n$$\n\nRather than using matrix-vector multiplication or an explicit matrix representation of the Jacobian, the forward mode of AD involves calculating ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ \n(i.e.  a single column of the Jacobian) by computing ${\\frac{\\partial f_i}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ for each of $i=1,...,m$ separately in one or more *forward \"passes\"*\n(also sometimes referred to as \"sweeps\").<sup>2,3</sup> If $f: \\mathbb{R}^1 \\to \\mathbb{R}^m$, only one forward pass would be needed to calculate the full Jacobian, since in that case $n=1$, the Jacobian matrix is $m$x$1$,\nand the seed vector would have a single entry. If $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then $n$ forward passes would be required (one forward pass for each entry in the $n$x$1$ seed vector). \nAlthough the matrix representation of the Jacobian is not explicitly used in the forward mode, this intuition carries over: within each forward pass, the derivative of\none of the variables is set to 1 and the rest to zero (for example, $\\dot{x}_1=1$ if we seek ${\\frac{\\partial f}{\\partial x_1}}$)\n\nAD works by breaking down the function $f$ into a composition of much simpler elemental functions (or elemental operations).\nEach forward pass starts by assigning 1 to the derivative of the variable $x_i$ for which we want to differentiate $f$ with respect to, and assigning 0 to all other variables.\nThen the elemental functions are evaluated from innermost to outermost, accumulating two numerical values at each intermediate variable $v_i$ in the trace<sup>3,4</sup>:\n* the **\"primal\"**: the value of the elemental function at $\\mathbf{x}=\\mathbf{a}$, denoted $v_i$ (i.e. the input of the elemental function will be carried over from any inner functions evaluated earlier in the trace)\n* the **derivative** or \"tangent\", denoted $\\dot{v_i}$ (i.e. the derivative of the elemental function evaluated at $\\mathbf{x}=\\mathbf{a}$)\n\nWithin each step, the derivative $\\dot{v_i}$ is implicitly calculated using the chain rule. For example, if the elemental function corresponding to $v_3$ results from the composition of several functions $w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x}))$ which for clarity we denote $w_3(u_1, u_2)$ then:\n$$\n\\dot{v_3} = {\\frac{\\partial}{\\partial x_1}} w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x})) = {\\frac{\\partial w_3}{\\partial u_1}}{\\frac{\\partial w_2}{\\partial x_1}} + {\\frac{\\partial w_3}{\\partial u_2}}{\\frac{\\partial w_1}{\\partial x_1}}\n$$\nIn forward mode, the innermost elemental function primals and derivatives are evaluated first and 'accumulated' outwards.\n\n## Software Organization\nBelow is a diagram of our directory stucture. \n\nThe main project directory contains the `build.sh` file. This file has the `cmake` and `make install` commands that build the test suite in `AutoDiff/tests/`. The `build.sh` file is specified in `.travis.yml`, enabling TravisCI to build our tests each time we `git push`. Our test suite also be utilized by `CodeCov` to show how much of our code we've hit.\n\nWe have two main ways to package our software: `cmake` and `make`. `CMake` is a lot nicer because the user has to do so much less in order to link up our library. `Make` requires us to configure the `VPATH` and locate the `.so` file. `CMake` requires less user configuration. \n\nOur `AutoDiff/` folder is what the user needs to have locally in order utilize our AutoDiff library. The user will need to `cd` into the `build/` folder, run the `cmake` command, then edit the `Makefile`, adding the `target_link_libraries` command to link up our library. Then, all the user needs to do is add `#include <AutoDiff.h>` at the top of their `.cpp` file, and `C++` will know where to find our AutoDiff library.\n\nWe have two modules: one for differentiation and one for root-finding, where root-finding actually uses the implementation of the differentiation module. A third module, a main module, will tie the two modules together.\n```\ncs107-FinalProject/\n            | README.md\n            | build.sh\n            | .travis.yml\n            | AutoDiff/\n                    | README.md\n                    | docs/\n                        | README.md\n                        | main_doc.md\n                        | Doxygen.config\n                        | images/\n                    | include/\n                        | AutoDiff.h\n                    | src/\n                        | AutoDiff_main.cpp\n                        | diff.cpp\n                        | roots.cpp\n                    | tests/\n                        | CMakeList.txt\n                        | test_main.cpp\n                        | src/\n                            | test1.cpp\n                            | test2.cpp\n                            | test3.cpp\n                    | lib/\n                    | build/\n```\n\n## How to Use\nWe assume that our user has followed procedures for downloading the `AutoDiff/` folder, run `cmake` within the `build/` folder, then linked the library by editing the `Makefile` to include `target_link_libraries`.\n\nNow that we've made and linked our libraries, `g++` compiler knows where to find the `AutoDiff` library. All they need to do is `include` the autodiff header file `AutoDiff.h` at the top of their `cpp` file where they intend to call/utilize the `AutoDiff` library.\n\nNext, the user has to declare some `autodiff types` (described in the implementation section). They construct a composite function of `autodiff types` and `.` call the differentiation and root-finding methods on that function, assigning it to some local variable for storagign the computation.\n\n## Implementation\n\nThe first part of our implementation is type and function declarations. These live in `AutoDiff.h`. At a high level, the user creates some function that is a composition of `autodiff types`. Since the user is expected to be working with `autodiff types`, we have to do no string parsing that would otherwise be necessary.\n\nWe define new `autodiff types` for the operators/functions `add`, `subtract`, `multiply`, `divides`, `sin`, `cos`, `tan`, `arcsin`, `arccos`, `arctan`, `power`, etc. We also declare functions for each of these function.\n\nThe user can feed in a variety of inputs, so we will be assuming that our the functions we define always take in numpy arrays (an array can be `1x1`). The user can ask for the derivative of a function, a numpy array (computing the Jacobian), or the function we are taking the derivative of can be evaluating a numpy array (ex derivative of `cos(array)` versus `cos(single variable)`. Our fundamental data structure is a numpy array.\n\nThe core of our diff implementation lives in `diff.cpp`, which takes in our `AutoDiff.h` header file.\n\nThe `autodiff type` variables will be seeded within `diff.cpp`. At the top of `diff.cpp`, we define .val and .dv instances containing a numpy array initializing the variable values and seeds (these arrays can be `1x1`).\n\nWe implement each function that we declared. Each functions takes in `autodiff types` (a numpy array of `autodiff types`) and numbers (`floats`), output two values: the function evaluation (as a numpy array) and the derivative evaluation (as a numpy array).\n\nA single function is a `1x1` case of the Jacobian, so we will presume that we are always outputting some matrix. For instance the function `F = (f, g)` should output (in pseudocode) `[[f.val | x1 and x2, g.val | x1 and x2], [[F.dfdx1, F.dfdx2], [F.dgdx1, F.dgdx2]]`, giving first the function evaluation as the first element and the derivative evaluation as the second element.\n\nThe function implementations \"sort of recursively\" call one another so that the inner most level is returned, allowing the next nested layer to get the `.val` and the `.dv`, up and up (like Russian dolls), until the outer most function is eventually computed. By this implementation, we essentially get the forward mode table that we did in class. Suppose we are fed in a matrix, we still return a matrix, but the implementation expands the functions inside in the correct way.\n\nLet's take an example of how the functions work. Suppose we have the function `sin(cos(x**2))` where `x.val` is seeded as `1.0` and `x.dv` is seeded as `1.0`. What forward mode does is expand the function derivative from inside out. So, first the `power` function implementation will return the function value and derivative value `[[x.val**2 = 1**2 = 1, 2*x.val*x.dv = 2*1*1 = 2]]` Next, the `cos` function returns `[[cos((x**2).val) = cos(1), -sin(x.val**2)*(x**2) = -sin((x**2).val)*((x**2).dv) = -sin(1)*2 = -2sin(1)]]`. Next the `sin` function returns `[[sin(cos(x**2).val) = sin(cos(1)), cos(cos(x**2).val)*cos(x**2).dv = cos(cos(1))*(-2sin(1))]]`.\n\nOnce our AD is implemented within `diff.cpp` we essentially just need to reference the functionality to write our root finder. The Newton-Raphson method is pretty short, and involves iterating once we acquire the function evaluation and derivative evaluation. The file `roots.cpp` will handle this iteration.\n\nLastly, we will wrap the diff implementation along with the root finder in a file called `AutoDiff_main.cpp`.\n\n## Testing\n\n* Location：under a folder name \"tests\" in the GitHub project repo\n\n* Tools:\n\n    * CodeCov: \n    \n        CodeCov provides statistics of code coverage so that each commit and pull request is measured and in control. \n        To do this, we need to create a .yaml file and specify our metrics (target at least 90%). \n        As our software covers two parts of functions: the basic function(AD implementation) and extensions, \n        it is reasonable to use tags in the .yaml file to separate coverage reports for different modules<sup>7</sup>. \n        Setting Codecov to send notifications to the Slack channel is also a good idea<sup>8</sup>, so everyone in the group \n        gets notified when there is an update.\n    \n    * TravisCI: \n        \n        TravisCI is a continuous integration platform that automatically builds and tests code changes. \n        Instead of merging large pieces of code and integrate them all in the end, we want to take a continuous \n        integration approach - merge in small code changes and get immediate feedback when each commit and pull \n        request happens<sup>9</sup>. We also need a .yaml file to use it. It can also send notifications through Slack. \n* Plan: \n    * Step 1: Exploratory Testing\n\n        Exploratory testing is a simultaneous process of test design and test execution all done at the same time, \n        contrarily to scripted testing, where all test cases are determined in advance<sup>10</sup>.\n        We will note down ideas about what to test and how it can be tested in a document file (.md). \n        We will update this document with additional test case designs, critically evaluate current test results with \n        respect to our expectations, and learn from the testing constantly.\n\n    * Step 2: Unit tests\n\n        Unit tests isolate issues that may arise and each unit is tested independently. \n        We will define the smallest unit to a each Class method. \n        By writing tests for the smallest testable units, then the compound behaviors between those, \n        we can build up comprehensive tests for more complex applications.<sup>11</sup>\n\n    * Step 3: Acceptance tests\n\n        After unit testings, we want to validate software against functional requirements/specifications. We mainly concentrate on:\n\n        * Functionality\n\n        * Basic Usability: Can users freely navigate through the screens without any difficulties?\n\n        * Error Conditions: Do suitable error messages display? Are they clear and proper?\n\n        * Accessibility\n\n\n        \n\n* Evaluations:\n\n    * Code coverage\n\n    * Risk analysis: What are the potential risks of current implementation? What are the important ones and what need to be addressed? \n\n    * Test execution log: recordings on the test execution\n\n## Documentations\n\n* Developer diary:\n\n    * Developer diary is an invaluable tool that helps the team remember bits of information that will otherwise get washed away. There is no need to keep it up to date. It can be records of: resources (url), insights (from success/failures), ideas, solutions, questions, and minutes.\n\n* Commit early, commit often\n\n* Software documentations: doxygen and graphviz packages\n\n* Demo examples: In the end, a few demo examples in the GitHub repo will let users get started easily. \n\n## Extensions\n\n### Dual Numbers\n\nDual number is an interesting type of number that uses the symbol $\\epsilon$ (epsilon) and offers how to do automatic\ndifferentiation in another persepective. It automatically calculates the derivative and the value of a function at the same time! \n\nDual numbers are pretty similar to imaginary numbers. A dual number has a real part and a dual part. We write\n\n$$\nx = a + b\\epsilon\n$$\n\nBut! For imaginary numbers, $i^2 = -1$, whereas for dual numbers, $\\epsilon^2 = 0$ (and $\\epsilon$ is not 0!). \n\n#### Basic Math\n\nAdd and subtraction of dual numbers are the same as adding complex numbers: we just add the real and dual parts separately.\n\n$$\n(3+6\\epsilon) + (1+2\\epsilon) = (3+1)+(6\\epsilon+2\\epsilon) = 4+8\\epsilon\n$$\n\n$$\n(3+6\\epsilon) - (1+2\\epsilon) = (3-1)+(6\\epsilon-2\\epsilon) = 2+4\\epsilon\n$$\n\nTo multiply dual numbers, we use F.O.I.L. just like we do with complex numbers\n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 6\\epsilon +6\\epsilon + 12\\epsilon^2 = 3 + 12\\epsilon + 12\\epsilon^2\n$$\n\nAs $\\epsilon^2=0$, the last item $12\\epsilon^2$ disappears. \n\n$$\n(3+6\\epsilon) \\times (1+2\\epsilon) = 3 + 12\\epsilon\n$$\n\nThe division process relates to the conjugate, like complex division - multiplying the denominator  by its conjugate \nin order to cancel the non-real parts.<sup>12</sup> The formula is given below:\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= \\frac{(a+b\\epsilon)(c-d\\epsilon)}{(c+d\\epsilon)(c-d\\epsilon)}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2 - d^2\\epsilon^2}\\\\\n&= \\frac{ac-ad\\epsilon+bc\\epsilon-bd\\epsilon^2}{c^2}\\\\\n&= \\frac{ac+(bc-ad)\\epsilon}{c^2}\\\\\n&= \\frac{a}{c}+\\frac{(bc-ad)\\epsilon}{c^2}\n\\end{aligned}\n$$\n\nwhich is defined when $c$ is non-zero. If, on the other hand, $c$ is zero while $d$ is not, then the equation\n\n$$\n\\begin{aligned}\n\\frac{a+b\\epsilon}{c+d\\epsilon} &= k\\\\\n\\frac{a+b\\epsilon}{d\\epsilon} &= k\\\\\na+b\\epsilon &= kd\\epsilon\\\\\n\\end{aligned}\n$$\n\n1. has no solution if $a$ is nonzero\n2. otherwise is solved by any dual number of the form $k=\\frac{b}{d}+y\\epsilon$. To expalins this, suppose we have $k=x+y\\epsilon$: \n\n$$\n\\begin{aligned}\nb\\epsilon &= kd\\epsilon\\\\\nb\\epsilon &= (x+\\epsilon y)d\\epsilon\\\\\nb\\epsilon &= xd\\epsilon + \\epsilon^2 y\\\\\nb\\epsilon &= xd\\epsilon\\\\\nx &= \\frac{b}{d}\\epsilon\n\\end{aligned}\n$$\n\nSo, $k=\\frac{b}{d}+y\\epsilon$ and is the value of $\\frac{a+b\\epsilon}{c+d\\epsilon}$.\n\n#### Using $\\epsilon^2$ for AD\n\nSuppose we have a function: $\nf(x) = 3x + 2$, and calculates $f(4)$ and $f'(4)$. \n\nWe know $4 = 4 + 1\\epsilon$, using $1$ for the dual component, since $x$ has a derivative of $1$.<sup>13</sup> We take this into the function:\n\n$$\nf(4) = f(4 + \\epsilon) = (4 + 1\\epsilon) \\times 3 + 2 + 1\\epsilon = (12 + 3\\epsilon) + 2 = 14 + 3\\epsilon\n$$\n\nWe can find out the real number component $(14)$ is the value of $f(4)$ and the dual component $(3)$ is the derivative $f'(4)$, \nwhich is verified below:\n\n$$\nf(x) = 3x+2 = 3\\times4 + 2 = 14\n$$\n\n$$\nf'(x) = 3\n$$\n\nIt is not an coincidence! Now try a quadratic example: $f(x)=5x^2 + 3x + 1$, also evaluates at $f(4)$.\n\n$$\n\\begin{aligned}\nf(4 + \\epsilon) &=5(4 + \\epsilon)^2 + 3\\times(4 + \\epsilon) + 1 \\\\\n &= 5(16 + 8\\epsilon+\\epsilon^2 ) + 12 + 3\\epsilon + 1\\\\\n &= 80 + 40\\epsilon + 5\\epsilon^2 + 12 + 3\\epsilon + 1\\\\\n &= (80+12+1) + (40+3)\\epsilon + 5\\epsilon^2 \\\\\n &= 93 + 43\\epsilon + 5\\epsilon^2\\\\\n &= 93 + 43\\epsilon\n\\end{aligned}\n$$\n\nNow verify it:\n\n$$\n\\begin{aligned}\nf(4)&=5\\times 4^2 + 3\\times 4 +1\\\\ &= 5\\times 16 + 12 + 1\\\\ &= 80 + 12 + 1 \\\\&= 93\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\nf'(x)&=10x + 3\\\\ \nf'(4)&=10\\times 4 + 3 = 43\n\\end{aligned}\n$$\n\nWe can also use dual number on trigonometric functions to do differentiation. \nWhat's more, it is not only limited to the 1st derivative.\nIt is possible to modify this technique to get the 2nd the 3rd, or the Nth derivative of a function. But in a 2nd derivative\nscenario, we need to define $\\epsilon^3=0$, instead of $\\epsilon^2=0$, so on and so forth.\nWe can also extend this to multivariable functions. To do this, we need to define $\\epsilon_x$, $\\epsilon_y$, ... for each variables.\n\nTo conclude, using dual numbers gives us exact derivatives, within the limitations of floating point math \n(compared to finite difference method). \n\n#### Implementation\n\nIt is tempting to create a \"DualNumber\" Class and overloads different operators$(+,-,*,/)$ and math functions, eg. sqrt, pow, sin, cos, tan, atan, ... \nWe keep all independent variables in a array-like container and in each overloading function, we loop through this array to calcualte derivative for each variables.\n\n### Root Finding and Nonlinear Sets of Equations\n\nThere are many useful applications of automatic differentiation and we would love to code them. As we get started, we do not want to be too ambitious so for this part, we only list out ideas. \nMore detailed plans and implementation may be presented in the next milestone.\n\n* Newton-Raphson Method using derivative: rapid local convergence but unpredictable global \nconvergence. We can visualize the convergence graph and see fractals, which refers \nto self-similar shapes across different scales\n\n* Halley’s Method (extension of Newton’s method)\n\n* Globally Convergent Methods for Nonlinear Systems of Equations\n\n### Solution of Linear Algebraic Equations\n\nCurrent popular methods of solving the problems below mostly take an iterative approach, \ne.g. Gauss-Jordan Elimination, Gaussian Elimination with Backsubstitution, LU Decomposition. Solving these problems also take large computation time; we are curious if automatic differentiation will give a better solution.\n\n* Solution of the matrix equation $Ax=b$ for an unknown vector $x$\n* Solution of more than one matrix equation $Ax_j=b_j$, for a set of vectors $x_j$, $j=0,1,..,$ each corresponding to \na different, known right-hand side vector bj\n* Calculation of the matrix $A^{-1}$ that is the matrix inverse of a square matrix $A$, i.e., $A^{-1}\\cdot A=A \\cdot A^{-1} =I$, \nwhere $I$ is the identity matrix   \n* Singular value decomposition of a matrix $A$\n* Linear least-squares problem\n\n    The reduced set of equations to be solved can be written as the $N\\times N$ set of equations\n    \n    $$\n    (A^{T}\\cdot A)\\cdot x = (A^{T}\\cdot b)\n    $$\n\n    where $A^{T}$ denotes the transpose of the matrix A.\n    These quations are called the normal equations of the linear least-squares problem. \n\nUsually, a direct solution of the normal equations is **NOT** generally the best way to find least-squares solutions.\nWe would like to see how automatic differentiation performs in this task and compare the differences. \n\n## References\n\n1. http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/AMLAutoDiff.pdf\n2. http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\n3. \"Automatic Differentiation in Machine Learning: A Survey\" https://www.jmlr.org/papers/volume18/17-468/17-468.pdf\n4. https://harvard-iacs.github.io/2020-CS107/lectures/lecture10/notebook/\n5. https://kenndanielso.github.io/mlrefined/\n6. https://people.cs.umass.edu/~domke/courses/sml/09autodiff_nnets.pdf\n7. https://docs.codecov.io/docs/flags\n8. https://docs.codecov.io/docs/notifications \n9. https://docs.travis-ci.com/user/for-beginners/\n10. https://www.guru99.com/exploratory-testing.html\n11. https://en.wikipedia.org/wiki/Unit_testing#:~:text=Unit%20tests%20are%20typically%20automated,an%20individual%20function%20or%20procedure\n12. https://en.wikipedia.org/wiki/Dual_number#:~:text=Division%20of%20dual%20numbers%20is,cancel%20the%20non%2Dreal%20parts\n13. https://blog.demofox.org/2017/02/20/multivariable-dual-numbers-automatic-differentiation/\n\n\n## Feedback\n### Milestone 1\n```\n- Root-finder extension sounds great! The dual numbers approach is also very interesting, but I'm struggling to imagine it cleanly written out for nonlinear operations. \n- I'd suggest prioritizing the root finder and adding the dual number approach later, although we can always discuss more. \n- Nice plan for testing! \n- Nice and clear software organization, but we would like to hear more about your choice of core data structure and the reasoning behind it.\n```","metadata":{"cell_id":"00001-7fade151-ed51-42d3-bceb-cd56475762b1","output_cleared":false,"tags":[]}},{"cell_type":"markdown","source":"## Changes\n### Milestone 1\n\n#### Dual Number Implementation\n\nJust like what stated above, dual number can be expressed as the followings:\n\n$$$\nz = a+b\\epsilon\n$$$\n$$$\n{\\epsilon}^{2} = 0\n$$$\n\nFor the purpose of auto differentiation, we can use dual numbers in this way:\n\n$$$\nz = x + x^{'}\\epsilon\n$$$\n\nNext, it gets very interesting\n\n| Expr. \\|  | Expr. Deri. \\| | Expr. w Dual Num.  \\| |   Expr. w Dual Num. Result     \\| | Real Part \\| | Dual Part |\n|:-----:|:--------------:|:-----:|:-----------------------:|:-------:|:-------:|\n|  $u+v$| $u^{'}+v^{'}$  |   $(u+u^{'}\\epsilon)+(v+v^{'}\\epsilon)$ | $u+u^{'}\\epsilon+v+v^{'}\\epsilon$  | $u+v$   | $(u^{'}+v^{'})\\epsilon$ |\n|  $u\\times v$     |    $(u^{'}v+vu^{'})$  |   $(u+u^{'}\\epsilon)\\times(v+v^{'}\\epsilon)$    | $uv + u^{'}v\\epsilon+vu^{'}\\epsilon+{\\epsilon}^{2}$   | $u\\times v$  | $(u^{'}v+vu^{'})\\epsilon$ |\n|  $u-v$| $u^{'}-v^{'}$  |   $(u+u^{'}\\epsilon)-(v+v^{'}\\epsilon)$ |   $u+u^{'}\\epsilon-v+v^{'}\\epsilon$| $u-v$  | $(u^{'}-v^{'})\\epsilon$ |\n|  $u\\div v$     |    $\\frac{(u^{'}v+vu^{'})}{v^2}$ |   $(u+u^{'}\\epsilon)\\div(v+v^{'}\\epsilon)$  |  $\\frac{uv+(u^{'}v-uv^{'})\\epsilon}{v^2}$   |   $u\\div v$      |   $\\frac{(u^{'}v+uv^{'})\\epsilon}{v^2}$      |\n| $\\sin(u)$ | $\\cos(u)u^{'}$ | $\\sin(u+u^{'}e)$  | $\\sin(u) +\\cos(u)u^{'}\\epsilon$ | $\\sin(u)$ | $\\cos(u)u^{'}\\epsilon$ | using $\\sin(u'\\epsilon)=u'\\epsilon,\\cos(u^{'}\\epsilon)=1 $\n| $\\cos(u)$ | $-\\sin(u)u^{'}$ | $\\cos(u+u^{'}e)$| $\\cos(u)-\\sin(u)\\epsilon$ |  $\\cos(u)$ | $-\\sin(u)\\epsilon$ |\n| $\\exp(u)$ | $\\exp(u)$ | $\\exp(u+u^{'})$ | $\\exp(u)+u^{'}\\exp(u)$ | $\\exp(u)$ | $u^{'}\\exp(u)$ | using Taylor series\n| $\\log(u)$ | $\\log(u+u^{'})$ | $\\log(u+u^{'}\\epsilon)$ | $\\log(u)+\\frac{u^{'}}{u}\\epsilon$ | $\\log(u)$ | $\\frac{u^{'}}{u}\\epsilon$ |\n| $u^{k}$   | ${ku^{'}u^{k-1}}$ |${(u+u^{'}u^{'}\\epsilon)}^{k}$  | $u^{k}+{ku^{k-1}}\\epsilon$|$u^{k}$ | ${ku^{'}u^{k-1}}$|\n\nSo, we can plan to make a Dual class and use a type name T to let it can be used as float, double ... etc.","metadata":{"tags":[],"cell_id":"00001-a4a5a3cb-486d-43cc-a7d1-5e9667263370","output_cleared":false}},{"cell_type":"code","metadata":{"cell_id":"00001-84e8fcb2-019d-4e4b-9c00-aacc79d840f1","output_cleared":false,"source_hash":"117cfa1a","execution_millis":2,"execution_start":1604460309013},"source":"template <typename T>\nclass Dual{\n    private:\n        T real;\n        T dual;\n};\n// Getters and Setters\nT getDual() const;\nvoid setDual(T dual);\n\n// Operators Overload (+,-,/,*)\ntemplate <typename T>\nDual<T> operator*(Dual<T> a, Dual<T> b){\n    return Dual<T>(a.real() * b.real(),a.real() * b.dual() + a.dual() * b.real());\n}\n\n// Standard Math\ntemplate <typename T>\nDual<T> sqrt(Dual<T> z){\n    T tmp = sqrt(z.real());\n    return Dual<T>(tmp, z.dual() * T(0.5) / tmp);\n}\ntemplate <typename T>\n\nDual<T> sin(Dual<T> z){\n    return Dual<T>(::sin(z.real), z.dual*::cos(z.real));\n}\n// cos, exp, log, abs, pow ...\n","execution_count":3,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-3-7a05c633036a>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7a05c633036a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    template <typename T>\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":"#### Root Finding and Optimization: Theory and Implementation\n\n##### Newton's method (also known as the Newton-Raphson Method):\n\n###### General properties and overview\n* second order optimality condition method in that the quadratic approximation for $f$ at a local point $a$ is used to compute the descent direction\n(direction in which to step to reach stationary points)\n* well-suited for optimizing or finding roots of convex functions with a moderate number of inputs (e.g. linear regression, logistic regression)\n* requires fewer steps to converge than methods which use linear (rather than quadratic) approximations of the function\n* with non-convex functions, the algorithm may converge to local maxima\n* as with other local optimization methods, starts from a single starting point and iteratively \"refines\" it by stepping in a descent direction\n* for optimization, the idea is to start at point $\\mathbf{a_0}$, go to $\\mathbf{a_1, a_2...a_n$ minimizing $f$: create second order Taylor series approx to $f$ at $a_i$ and travel to stationary point of quadratic\n\n###### Theory\nBuilding from simple to more general cases of the functional form of $f$...\n\n*$f: \\mathbb{R} \\to \\mathbb{R}$*\n$$\nf(x) = f(x)\n$$\n* the second order Taylor approx. of $f$ at $\\mathbf{a}$ is: \n$$g(x) = f(a) + \\frac{d}{dx} f(a) (x-a)+\\frac{1}{2}\\frac{d^2}{dx^2}f(a)(x-a)^2$$\n* we can solve for the stationary point of the second order Taylor approx. of $f$ by setting its derivative to 0:\n$$\n\\begin{aligned}\n\\frac{d}{dx}g = \\frac{d}{dx}f(a) + \\frac{1}{2}\\frac{d^2}{dx^2}f(a)(2x^{*} - 2a) &= 0\\\\\n\\frac{d^2}{dx^2}f(a)(x^{*} - a) &= -\\frac{d}{dx}f(a)\\\\\nx^* &= a - \\frac{\\frac{d}{dx}f(a)}{\\frac{d^2}{dx^2}f(a)}\\\\\n\\end{aligned}\n$$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}$*\n$$\nf(\\mathbf{x})= f(x_1, ..., x_n)\n$$\n* the second order Taylor approx of $f$ at $\\mathbf{a}$ is: \n$$g(\\mathbf{x}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n* analogously to the above, solving for the stationary point of the second order Taylor approx. of $f$ yields:  \n&nbsp;&nbsp;&nbsp;&nbsp;$$\\mathbf{x}^* = \\mathbf{a} - (\\nabla^2 f(\\mathbf{a}))^{-1} \\nabla f(\\mathbf{a})$$  \nwhere $\\nabla^2 f(\\mathbf{a})$ is the Hessian of $f$ at $\\mathbf{a}$\n\n*$f: \\mathbb{R}^n \\to \\mathbb{R}^m$*\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n* the second order Taylor approx of the $i^{\\text{th}}$ element of $f$ at $\\mathbf{a}$ is: \n$$g_i(\\mathbf{x}) = f_i(\\mathbf{a}) + \\nabla f_i(\\mathbf{a})^{T}(\\mathbf{x}-\\mathbf{a})+\\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^{T}\\nabla^2f_i(\\mathbf{a})(\\mathbf{x}-\\mathbf{a})$$\n\n###### Implementation for optimization\nInputs: $f(\\mathbf{x})$, max steps $m$, initial value $\\mathbf{a_0}$\nOutputs: $[a_0, a_1, a_2...a_k]$, $[f(a_0), f(a_1), ... f(a_k)]$\n\nPseudocode:\n* get value and gradient of $f$ from AD implementation\n* $\\text{for } i \\text { in } [1,2, ...k]:$\n* $a_i = a_{i-1} - (\\nabla^2 f(a_{i-1}))^{-1}\\nabla f(a_{i-1})$\n\n##### References:\nhttps://kenndanielso.github.io/mlrefined/blog_posts/7_Second_order_methods/7_3_Newtons_method.html\n\nhttps://www.math.usm.edu/lambers/mat419/lecture9.pdf\n\nhttp://fourier.eng.hmc.edu/e176/lectures/NM/node45.html","metadata":{"tags":[],"cell_id":"00003-03a92ab6-e4f5-4eec-b748-e7542df59037","output_cleared":false}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00004-740493a9-491d-4341-9f92-8bb0cd399327","output_cleared":false}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"2b89fca5-1277-4d61-83f1-89a103dbb355","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}