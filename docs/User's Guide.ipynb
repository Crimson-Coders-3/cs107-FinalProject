{"cells":[{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00001-d957caea-19f4-46cb-824c-ff78d003af32","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":" # User's Guide","metadata":{"tags":[],"cell_id":"00002-eddef084-7d33-4113-92f0-eb9a7ff3a5dd","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Installation Guide / Getting Started (TL;DR)\n\nThis is a User Guide to our Auto Differentiation, Root-finding, and Dual Number Libraries.\n\n### 1.1 Installation\n\n1. Clone our GitHub repo:\n```\ngit clone https://github.com/Crimson-Coders-3/cs107-FinalProject/\n```\n\n2. Build third party libraries:\n```\ncd 3PL\n./build_3PL.sh\n```\n3. Execute config.sh from the main project directory: \n```\ncd ..\n./config.sh\n```\n\n### 1.2 Start Coding\n4. To start coding:\n```\ncd App\ncd src\nvim main.cpp\n```\nIn `App/src` you will find `main.cpp` preconfigured for you to call any of the Auto Differentiation, Roots, and Dual libraries.\nYou may notice that `main.cpp` includes only the roots library (`#include \"Roots.h\"`). \nRoots uses ADFunc and ADFuncVector, so including just Roots catches all three in one.\n\nWorking in `App/src`, you should have no problem creating other C++ files and including the header file of the relevant library, you are calling.\nI.e., `#include \"ADFunc.h\", #include \"ADFuncVector.h\", #include \"Dual.h\", #include \"Roots.h\"`.\n\n### 1.3 Compile and Execute\n\n5. To compile your C++ code within `App/src`, edit the `App/src/CMakeLists.txt` file to include any additional `.cpp` files you created.\n```\nnano CMakeLists.txt\n```\n6. Now return the `App` directory and run `./config.sh`\n```\ncd ..\n./configure.sh\n```\n7. To run your executable, enter the `App/bin` directory and run your executable.\n```\ncd bin\n./app_demo\n```\n\n### 1.4 Calling Functions / Function Documentation\n\n8. `App/src/example.cpp` includes some function declarations to get you started.\n```\n#include <math.h>\n#include <string>\n#include <cmath>\n#include \"Roots.h\"\n\nusing namespace std;\n\nvector<ADFunc> customFunct2(Eigen::VectorXd vals){\n     ADFunc x = ADFunc(vals[0], {1,0,0});\n     ADFunc y = ADFunc(vals[1], {0,1,0});\n     ADFunc z = ADFunc(vals[2], {0,0,1});\n     return {5*sin(x), 3*y/2, z+2};\n}\n\n\nint main(){\n    function<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct2;\n    Equation eq(customFunct2, 3);\n    Eigen::VectorXd guess(3);\n    guess << 1, 1, 1;\n    double tol = 1e-4;\n    Eigen::VectorXd roots = eq.getRoots(guess, tol);\n    cout << \"Roots found: \\n\" << roots << endl;\n\n    //Example of getting the Jacobian (as a MatrixXd object)\n    ADFunc x = ADFunc(7, {1,0});\n    ADFunc y = ADFunc(3, {0,1});\n    vector<ADFunc> F2 = {2*pow(x,2), 5*sin(y)*x};\n    Eigen::MatrixXd J = getJ(F2, 2);\n    cout << \"Jacobian of F: \\n\" << J << endl;\n    return 0;\n}\n```\n9. A detailed list of all function declarations and functionality description is in our Doxygen.\nWhen you `git clone`-ed our library, you automatically received our Doxygen documentation.\nFrom the main directory, navigate to `docs/GENERAETED_DOCS/html/index.html`\n```\ncd docs\ncd GENERAETED_DOCS/html\nClick on `docs/GENERAETED_DOCS/html/index.html` and it will launch locally in your browser.\n```\n10. Navigate to `Files/File List`. Click on the file of interest to you, and scroll down to \"Functions.\"\nClick on any of the functions to arrive at the Reference page, where you will find a description of all that library's functions.\n\n### 1.5 Requirements:\n\nHere are some requirements you will need. Also see \"4.3 Possible Error Messages\" in you run into a proble,..\n\n1. cmake:\n\n* tutorial: https://cliutils.gitlab.io/modern-cmake/chapters/intro/installing.html\n* documentation: https://cmake.org/cmake/help/v3.19/\n* useful links: https://github.com/ackirby88/CS107/tree/master/cmake\n\n2. Eigen (linear algebra library)\n\n* overview: http://eigen.tuxfamily.org/dox/index.html\n* source code bundled with our GitHub repo under \"3PL\" (containing the extracted tar.gz file from http://eigen.tuxfamily.org/index.php?title=Main_Page#Download)\n* *following step 2 in the Installation Guide above will build Eigen by default, to the `install` directory*\n\n3. GoogleTest (for testing only, not required)\n\n* additional info.: https://github.com/google/googletest/tree/master/googletest\n* *following step 2 in the Installation Guide above will build GoogleTest to the `install` directory*\n\n4. lcov (for testing only, not required)\n\n* *following step 2 in the Installation Guide above will build lcov by default, to the `install` directory*\n* once the installation completes, you may try \"make test\" to verify \nif it is installed correctly\n* additional info: http://ltp.sourceforge.net/coverage/lcov.php\n* source code repo: https://github.com/linux-test-project/lcov\n\n5. C++ 11\n\n### 1.6 Beyond\n\nIf you need any help using our library or desire to provide feedback (which we are always very eager for!) contact us here:\n- Scarlett Gong: wenlin_gong@g.harvard.edu \n- Morris Reeves: morrisreeves@g.harvard.edu \n- Gayatri Balasubramanian: gayatrib@college.harvard.edu\n\n### 1.7 Broader Impact and Inclusion Statement\n\nSee our mission and principles in section 2 (right below).\n\n## 2 Broader Impact and Inclusion\n\n### 2.1 Broader Impact\nBroader Impact\nFrom Linux to Python and R, open source was one of the most ingenious consequences of the internet. It was a larger-than-life platform to exchange code and come together to build better, more comprehensive software. However, open source has been a platform of something else: an extension of the biases and inequalities in our own society. Perhaps compounded even more by how rapidly open-source code and software develop – like a kid chasing an accelerating train. From spare or difficult documentation to difficult syntax and a lack of transparency in the code, open source software can be impassible. As develops, our foremost intention is to make our software accessible to the world, equally.\nHere, we developed an open-source auto-differentiation library in C++, a field and a programming language both not very accessible to those most affected by the inequalities in computing.  Our strength in making our library transparent and accessible actually came from our weakness. Two of us began this project with no C++ background and one of us had a single year of experience. We experienced firsthand the impassibility of this content – we even watched our teaching staff falter. So, while the process of developing this library was uphill for us, it taught us – made transparent to us – where our users will struggle as well.\nWe acknowledge that software, by definition, has a barrier, because nobody can learn to code nor read code overnight. But we aimed to make an accessible C++ auto-differentiation library through the eyes of our users.\nA user navigates to open source code on GitHub and first navigates to the README.md file. Striving for software inclusivity, we wrote extensive, readable documentation. We organized our documentation as readably as possible. We included a “tl;dr” because often this is the first thing a user looks for. After following documentation to download, a user often next looks for common sense folder names telling them where to begin. Our “Example” folder containing “example.cpp” sets all this up for the user. Next, a user looks for the methods/functions in the library, and our “.h” header files contain extensive comments.\nWe hope that through thinking through the user’s thought process, we have anticipated and understood the diversity of backgrounds they have. We included our contact info, as users who need help with our documentation can reach out to us directly for help. And to those on the more developer side, we included the ability to make pull requests, which we review on a frequent basis.\nWe acknowledge that our software, like all software, can be abused. As a programmer, you must understand that being a good citizen also means being an ethical coder. If you have doubts about usage, reach out to us. While accessible and free of charge, we have the right to notify law enforcement if you use our code for harm.\n\n### 2.2 Inclusivity Statement\nCrimson Coders 3 welcome and encourage you to use and contribute to our auto-differentiation library. Our work is guided by the principles of equality, transparency, respect, giving, and kindness. To us, diversity is not only a multi-identity user community, but the accessibility of our code to the world. We encourage you to contribute if you can or reach out if you need help understanding our library.\n\n<a id='Introduction'></a>\n## 3. Introduction: Our Library and Motivation\n\nThe purpose of our software is to implement a C++ library for Automatic Differentiation through forward mode, Automatic Differentaition through dual numbers, and multi-dimensional Root-Finding.\nOur library provides the efficient function evaluation and derivative evaluation at machine precision for any real-valued or vector-valued function.\n\n### 3.0 Why Forward Mode and Dual Numbers\nWhile we chose forward mode and dual numbers to implement automatic differentiation, there are two main alternatives to automatic differentiation (\"AD\"): symbolic differentiation and numerical (approximation) methods.\nWhile symbolic differentiation evaluates derivatives to machine precision, it is often costly to evaluate and prone to 'expression swell': \nsymbolic differentiation may result in exponentially large expressions to evaluate. In contrast, numerical methods (such as the finite difference method) are \nquick to evaluate but do not generally result in machine precision: when evaluating $(f(x+\\epsilon)-f(x))/\\epsilon$, use of a large epsilon results in imprecise approximation,\nwhereas use of a small epsilon results in floating point errors. Moreover, the complexity of numerical methods increases as the number of dimensions of the gradient increases.<sup>3</sup>\nAutomatic differentiation circumvents these challenges by \"accumulating\" numerical evaluations of simple elementary functions.\n    \n### 3.1 Motivation\nThe efficient evaluation of derivatives at machine precision naturally has a wide (and growing) range of application areas, of which a few are highlighted below:\n* Differentiation of functions implemented as code: AD does not require closed-form expressions (as is the case in symbolic differentiation)<sup>3</sup>\n* Backpropagation: training neural networks requires finding weights that minimize the objective function\n    * AD has seen increasingly widespread use since ~2015 as deep learning grows in popularity (e.g. AD implementations in TensorFlow and PyTorch)<sup>3</sup>\n    * Applications include computer vision, NLP\n* Solving ODEs\n* Gradient-based optimization (finding minima through step-wise updates)<sup>3</sup>\n* Root finding, including Newton's method\n\nAD is an important problem not only because it is elegant, but it sits at the heart of many of the interesting questions in computation and real-world problems.\n\n<a id='Background'></a>\n## 4. Background: Mathematics and Intuition Behind AD\n\nSuppose we have a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    f_1(x_1, ..., x_n) \\\\\n    f_2(x_1, ..., x_n) \\\\\n    \\vdots \\\\\n    f_m(x_1, ..., x_n)\n\\end{bmatrix}\n$$\n\nand suppose that we seek ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$.\n\nIf we have $J_f$, the Jacobian matrix of $f$, we can compute ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ as the product of the Jacobian matrix and the desired seed vector $s$:\n\n$$\nJ_f\\mathbf{s}= \\begin{bmatrix}\n    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\dots & \\frac{\\partial f_1}{\\partial x_n} \\\\[1ex]\n    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\dots & \\frac{\\partial f_2}{\\partial x_n} \\\\[1ex]\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\[1ex]\n    \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\dots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\[1ex]\n0 \\\\[1ex]\n\\vdots \\\\[1ex]\n0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} \\\\[1ex]\n\\frac{\\partial f_2}{\\partial x_1} \\\\[1ex]\n\\vdots \\\\[1ex]\n\\frac{\\partial f_m}{\\partial x_1}\n\\end{bmatrix}\n$$\n\nRather than using matrix-vector multiplication or an explicit matrix representation of the Jacobian, the forward mode of AD involves calculating ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ \n(i.e.  a single column of the Jacobian) by computing ${\\frac{\\partial f_i}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ for each of $i=1,...,m$ separately in one or more *forward \"passes\"*\n(also sometimes referred to as \"sweeps\").<sup>2,3</sup> If $f: \\mathbb{R}^1 \\to \\mathbb{R}^m$, only one forward pass would be needed to calculate the full Jacobian, since in that case $n=1$, the Jacobian matrix is $m$x$1$,\nand the seed vector would have a single entry. If $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, then $n$ forward passes would be required (one forward pass for each entry in the $n$x$1$ seed vector). \nAlthough the matrix representation of the Jacobian is not explicitly used in the forward mode, this intuition carries over: within each forward pass, the derivative of\none of the variables is set to 1 and the rest to zero (for example, $\\dot{x}_1=1$ if we seek ${\\frac{\\partial f}{\\partial x_1}}$)\n\nAD works by breaking down the function $f$ into a composition of much simpler elemental functions (or elemental operations).\nEach forward pass starts by assigning 1 to the derivative of the variable $x_i$ for which we want to differentiate $f$ with respect to, and assigning 0 to all other variables.\nThen the elemental functions are evaluated from innermost to outermost, accumulating two numerical values at each intermediate variable $v_i$ in the trace<sup>3,4</sup>:\n* the **\"primal\"**: the value of the elemental function at $\\mathbf{x}=\\mathbf{a}$, denoted $v_i$ (i.e. the input of the elemental function will be carried over from any inner functions evaluated earlier in the trace)\n* the **derivative** or \"tangent\", denoted $\\dot{v_i}$ (i.e. the derivative of the elemental function evaluated at $\\mathbf{x}=\\mathbf{a}$)\n\nWithin each step, the derivative $\\dot{v_i}$ is implicitly calculated using the chain rule. For example, if the elemental function corresponding to $v_3$ results from the composition of several functions $w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x}))$ which for clarity we denote $w_3(u_1, u_2)$ then:\n$$\n\\dot{v_3} = {\\frac{\\partial}{\\partial x_1}} w_3(w_2(\\mathbf{x}), w_1(\\mathbf{x})) = {\\frac{\\partial w_3}{\\partial u_1}}{\\frac{\\partial w_2}{\\partial x_1}} + {\\frac{\\partial w_3}{\\partial u_2}}{\\frac{\\partial w_1}{\\partial x_1}}\n$$\nIn forward mode, the innermost elemental function primals and derivatives are evaluated first and 'accumulated' outwards.\n\n## <a id='Software_Organization'></a>\n## 5. Software Organization: Directory Structure, Basic Modules, Testing, and [Installation](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md)\n\n### 5.0 Directory Structure\n\nThe files most relevant to the reader are marked with *s below -- particularly these are `.cpp` and `.h` files. \nThe header files contain function declaratations, meaning that if they user took a look a these files, they would \nhave a good syntatic idea of how to construct and run class methods.\n\nThe folders are organized in this particular way to enable GoogleTest and CMake, and integration with TravisCI and CodeCov.\n**As a user, if you follow the installation guidelines, you will not have to worry about anything except the `App` folder.\nAs a developer, checkout the \"Testing\" and \"Developers\" sections below, as you will have to think about a few more file (the `.sh` files, that is).**\n\nFile structure (* marks files/folder relevant to the user; ^ marks files relevant to developer)\n```\n/cs107-FinalProject\n    + /3PL\n        + googletest-release-1.8.1\n        + lcov\n        | ^build_3PL.sh\n    + /App\n        + /src\n            *CMakeLists.txt\n            **main.cpp\n        + /tests\n        | CmakeLists.txt\n        | ^config.sh\n    + /AutoDiff\n        + /core\n            + /src\n                | *ADFunc.cpp\n                | *ADFuncVector.cpp\n                | CMakeLists.txt\n            + /tests\n                | test_main.cpp\n                | CMakeLists.txt\n                | src\n                    | test_vars.h\n                    | test_ADF_nested.cpp\n                    | test_ADF_unit.cpp\n                    | test_ADF_names.cpp\n                    | test_ADFV_unit.cpp\n        + /include\n            *ADFunc.h\n            *ADFuncVector.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /Dual\n        + /core\n            + /src\n                | *Dual.cpp\n                | CMakeLists.txt\n            + /tests\n            | CMakeLists.txt\n        + /include\n            *Dual.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /Example\n        | Makefile\n        | example.cpp\n    + /Roots\n        + /core\n            + /src\n                | CMakeLists.txt\n                | *Roots.cpp\n            + /tests\n            | CMakeLists.txt \n        + /include\n            | *Roots.h\n        | CMakeLists.txt\n        | ^config.sh\n        | ^coverage.sh\n    + /docs\n        + images\n        | README.md\n        | milestone1.ipynb\n        | milestone2.ipynb\n        | milestone2_progress.ipynb\n        | *^documentation.ipynb\n        | slides.ppt\n    | .travis.yml\n    | *^README.md\n    | ^codecov.yml\n    | ^config.sh\n```\nWe use GoogleTest to generate local code coverage reports. We use `converge.sh` files to make it easier to call `lcov` and link coverage info to an `index.html` under a generated `coverage` folder. \nTo call `converge.sh`, the user may run `./config.sh -gtest` in the root directory, or run the local coverage report `./coverage.sh`.\n\n### 5.1 Basic Modules\n\nOur functions per library (AutoDiff, Dual, and Roots) are located in doxygen. We cannot link this for you, as doxygen is run locally on the user's machine offline.\n\nTo view the doxygen documentation, `cd` from the root directory in `docs/GENERAETED_DOCS/html`. Then launch `docs/GENERAETED_DOCS/html/index.html`.\nTo view the doxygen documentation, `cd` from the root directory in `docs/GENERAETED_DOCS/html`. Then launch `docs/GENERAETED_DOCS/html`.\n```\ncd docs/GENERAETED_DOCS/html\nThen click on index.html\n```\nThis will launch locally in your browser.\nNavigate to Files/File List. Click on the file of interest to you, and scroll down to \"Functions.\"\nClick on any of the functions to arrive at the Reference page, where you will find a description of all that library's functions and their specs.\n\n### 5.2 Testing \nWe testing using GoogleTest. This means that we aim to not only cover every function with our tests, but every line of code as well.\nAs a developer, `cd` into the folder you are working in (`AutoDiff, Dual, Roots`).\n```\ncd AutoDiff\n```\nEnter the `core/tests/src` folder. Here you will see a bunch of `_test_.cpp` files.\n```\ncd core/tests/src\nls\n```\nWrite your own tests in this folder. When you are ready to run, head back to the root `AutoDiff, Dual, Roots`.\nRun `./config.sh` and then `./coverage.sh` (sometimes just running the coverage bash file is not enough to refresh the system that a change has taken place).\n```\ncd ..\ncd ..\ncd ..\n./config.sh\n./coverage.sh\n```\nThanks to GoogleTest, this process has automatically generated an html index page that we can visit and check the coverage and unit tests of our `.cpp` files.\n`cd` into `coverage/CODE_COVERAGE` and launch `index.html` in your local browser by double clicking on it.\n```\ncd coverage/CODE_COVERAGE\nclick on index.html to launch it\n```\nBy looking at the coverage statistics and line by line highlighting, we knew where we had enough unit tests and we needed more.\nWe checked each derivative and evaluation by hand using Wolfram Alpha, allowing up to verify that each function was working as intended.\nThe `core/tests/src` give an excellent illustration of how to complete tests, and we refer the user to these if they intend to do development with our work.\n\n### 5.3 Installation\n\nWe refer you to our [installation guide](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md).","metadata":{"tags":[],"cell_id":"00002-64814b16-be69-43eb-8ac5-c5bf3ee84c2f","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 6. Tutorials / How to Use\n\n**Note that most of our documentation about implementation and usage is in doxygen. Only a sampling is replicated here for your reference.**\n\n### Configuration\n\nWe again refer you to our [installation guide](https://github.com/Crimson-Coders-3/cs107-FinalProject/tree/master/README.md).\n\nAs a user you just need to follow those instruction and work in `App/src`. The files relevant to you are *-ed in the \"Software Organization\" section, if you need a visual cue.\nAs a developer, refer to the \"Software Organization\" section for a visual layout of which files matter to you, and for a description of how to run tests.\n\nRemember, when you are developing and add functionality or new files, make sure that the `coverage.sh` and `config.sh` files are tracking your additions.\nMake sure you write comprehensive test in the `core/tests/src` folder as detailed in the \"Testing\" section above.\n\n**Note: If you get a \"permission denied\", you need to check if config.sh, coverage.sh, or Makefile is executable. Type \"chmod +x \\[executable file\\]\" in the command prompt to add files to be executable.**\n\nOther common problems in the \"Troubleshooting\" Section.\n\n### 6.0 Quickstart with executing example code:\n\n1. Navigate to the `Example` directory\n2. Run `make` within the `Example` directory to build the executable for `example.cpp`\n3. Execute: `./example`\n\n### 6.1 Quick example of using our library to find ${\\frac{\\partial f}{\\partial x_1}}|_{\\mathbf{x}=\\mathbf{a}}$ for $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$\n\nSuppose we have the following function $f(\\mathbf{x})$ and we seek ${\\frac{\\partial f}{\\partial x_1}}$ at $x_1 = 1$, $x_2 = 5$:\n$$\nf(\\mathbf{x})=\n\\begin{bmatrix}\n    sin(x_1)+cos(x_2) \\\\\n    2x_1 + 3x_2 \\\\\n\\end{bmatrix}\n$$\n\nTo begin, we instantiate a vector of doubles holding the values at which we wish to evaluate the derivative:\n\n`std::vector<double> init_values = {1.0, 5.0};`\n\nWe then initialize a vector of `ADFunc` objects, and use the `multiVar` function to automatically set the seed of $x_1$ to $[1, 0]$ and the seed of $x_2$ to $[0,1]$:\n\n```\nstd::vector<ADFunc> multi_vars = multiVar(init_values);\nADFunc x_1 = multi_vars[0];\nADFunc x_2 = multi_vars[1];\n```\n\nWe may now define the functions $f(\\mathbf{x})$ above by defining $f_1$ and $f_2$ and defining a vector of ADFunc objects:\n\n```\nADFunc f1 = sin(x_1)+cos(x_2);\nADFunc f2 = 2*x_1 + 3*x_2;\nstd::vector<ADFunc> F = {f1, f2};\nADFuncVector Fvec(F);\n```\n\n### 6.2 Seed Vector\n\nIf you define a function that uses multiple variables, for each variable, when declaring ADFunc object you\nneed to give them a seed vector. For example if you have 3 variables: x, y, z.\n    \n    #include $<$vector$>$ \n    \n    std::vector<double> seed_x {1.0, 0.0, 0.0};\n\n    std::vector<double> seed_y {0.0, 1.0, 0.0};\n\n    std::vector<double> seed_z {1.0, 0.0, 1.0};\n\n    ADFunc x(2.0, seed_x);\n\n    ADFunc y(3.0, seed_y);\n\n    ADFunc z(4.0, seed_z);\n\nNote: seed vectors don't have to be unit vectors! Though in most cases, we use them as unit vectors as $\\frac{d}{dx}=1$\n\nYou can change the seed vector by using set_seed(). \n\n1. You can change a value in a seed vector: void ADFunc::set_seed_wrt(int index, double dval) \n\n2. You can change a seed vector as a whole: void ADFunc::set_seed(std::vector<double> dvals)\n\n### 6.3 Basic Operations and Elementary Functions\n\nFor operations below, please refer to Doxygen\n\n* Addition (commutative)\n\n* Subtraction\n\n* Multiplication (commutative)\n\n* Division\n\n* Power\n\n* Negation\n\n* Trig functions \n\nsine, cosine, tangent\n\n* Inverse trig functions \n\narcsine, arccosine, arctangent\n\n* Exponentials\n\nShould be able to handle any base\n\n* Hyperbolic functions \n\nsinh, cosh, tanh\n\n* Logistic function\n\n* Logarithms\n\nNote in C++ there is no log(x,base), which is different from Python. \nSo you have to use division manually if you want to calculate at other bases, \ne.g.`log(x)/log(5)` if you want to calcualte $log_5{x}$\n\n* Square root\n\n\n### 6.4 Comparison Operators\n\n* == and !=\n\n* \\>,<,>=,<=\n\n### 6.5 Operators NOT Supported in ADFunc Clas\n\nArithmetic: %        \n\nIncrement & Decrement: ++, --     \n\nCompound Assignment: %=, >>=, <<=, &=, ^=, |=  \n\nLogical: !, &&, ||                   \n\nConditional ternary: ?                        \n\nExponential & Logarithmic: frexp, ldexp, modf, ilogb, logb, scalbn, scalbln\n\nTrigonometric: atan2 \n\nError & Gamma functions: erf, erfc, tgamma,  lgamma  \n\nRounding & Remainder: ceil, floor, fmod, trunc,round, lround,llround,rint,\nlrint, nearbyint, remainder \n                     llrint, remquo             \nFloating-point manipulation: copysign, nan,      \n                          nextafter, nexttoward  \n\nMin, Max, Difference: fdim, fmax, fmin          \n\nOthers: fabs, abs, fma                           \n\n### 6.6 Name Mode\n\n#### 6.6.1 General Usages\n\nIf you want to each variable to have a variable name, you can do this by using\n\n1. **Constructor**: ADFunc::ADFunc(double val, std::vector<double> seed, std::vector<std::string> var_names)\n\n    where var_names is a vector of strings. Example:\n\n    >#include $<$vector$>$\n    \n    > std::vector<std::string> names {\"x\",\"y\"};    \n    \n    >std::vector<double> seed {1.0, 2.0};\n    \n    >ADFunc example = ADFunc(0.2, seed, names);\n\n2. **Turn name mode on**: void ADFunc::setName()\n\n    >#include $<$vector$>$ \n    \n    > std::vector<double> seed {1.0, 2.0};\n    \n    > ADFunc example = ADFunc(0.2, seed); // not in name mode\n\n    > example.setName(); // in name mode now\n\n    This function will have no impact if the variable is already in name mode.\n\n3. **Name Vector**: void ADFunc::setName(std::vector<std::string> var_names)\n\n    Note: It is okay before calling this function, the variable is not in name mode. After calling this function\n    and all the variables are assigned by `var_names` vector, the name mode is automatically turn on.\n\n    >#include $<$vector$>$ \n    \n    > std::vector<double> seed {1.0, 2.0};\n    \n    > ADFunc example = ADFunc(0.2, seed); // not in name mode\n\n    > std::vector<std::string> names {\"x\",\"y\"};  \n\n    > example.setName(names); // in name mode now\n\n4. **Modifier**: void ADFunc::setName(int index, std::string var_name) \n\n    > #include $<$vector$>$ \n    \n    > std::vector<double> seed {1.0, 2.0};\n\n    > std::vector<std::string> names {\"x\",\"y\"};  \n\n    > ADFunc example = ADFunc(0.2, seed, names);\n    \n    > example.setName(1,\"z\"); // now, the name vector becomes {\"x\",\"z\"}\n\nFor a particular variable, its name vector must follow the rules below: \n\n**1. all the names are unique**\n\n**2. size of name vector = size of seed vector**\n\nThe benefit of using name mode is that you can get partial derivative easier\n\n    > #include $<$vector$>$ \n    \n    > std::vector<double> seed {1.0, 2.0};\n\n    > std::vector<std::string> names {\"x\",\"y\"};  \n\n    > ADFunc example = ADFunc(0.2, seed, names);\n    \n    > std::cout << example.dval_wrt(\"x\"); // this should give 1.0\n\n#### 6.6.2 Name Conversion\n\nWe suggest that once you use a variable in name mode, you declare all the variables in name mode. You \ncan expect that in some cases, name conversion happen automatically; however, in other cases, it will\ngenerate warning or error messages. A summary is as below\n\n### 6.7 Root Finder Extension\n\nThe Root Finder is implemented in `Roots`. Its base in an Equation class whose constructor initializes a function and number of variables.\n\nThe class overloads `=` and `<<` to showing and store information about the class attributes. \n\nThe Roots library uses `Eigen` in its vector-getter method, and `Eigen` again its Jacobian evaluation.\n\nThe core of the Roots library is the `getRoots` method, which takes an `Eigen` vector and double to iterate and output roots.\n\nHere are examples of usage:\n```\n# create a custom function\nvector<ADFunc> customFunct(Eigen::VectorXd vals){\n    ADFunc x = ADFunc(vals[0], {1,0});\n    ADFunc y = ADFunc(vals[1], {0,1});\n    return {5*pow(x,2)+2*y, 3+y};\n}\n\n# access the public variable num_vars\nfunction<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct;\nEquation eq = Equation(customFunct, 2);\nint number_of_variables = eq.num_vars;\n\n# get values from vector of ADFunc objects returned by function(values)\nfunction<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct;\nEquation eq(customFunct, 2);\nEigen::VectorXd values(2);\nvalues << 2, 3;\nstd::vector<ADFunc> F = eq.function(values);\nint value = F[0].val();\n\n# get values from the Eigen::VectorXd object from conversion via getF \nfunction<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct;\nEquation eq(customFunct, 2);\nEigen::VectorXd values(2);\nvalues << 2, 3;\nEigen::VectorXd Fv = getF(eq.function(values));\nint value = Fv(0);\n\n# get Jacobian externally\nADFunc x = ADFunc(2, {1,0});\nADFunc y = ADFunc(3, {0,1});\nstd::vector<ADFunc> F = {5*pow(x,2)+2*y, 3+y};\nEigen::MatrixXd J = getJ(F, 2);\nint df1dx = J(0,0);\nint df1dy = J(0,1);\nint df2dx = J(1,0), 0);\nint df2dy =J(1,1), 1);\n\n# get roots\nfunction<vector<ADFunc>(Eigen::VectorXd)> Func = customFunct2;\nEquation eq(customFunct2, 3);\nEigen::VectorXd guess(3);\nguess << 1, 1, 1;\ndouble tol = 1e-4;\nEigen::VectorXd roots = eq.getRoots(guess, tol);\nstd::vector<ADFunc> F = eq.function(roots); //check that F(roots) near 0\nEigen::VectorXd F_at_roots = getF(F);\ndouble l2_norm = F_at_roots.squaredNorm();\nstd::cout << \"L2 norm: \" << l2_norm << std::endl;\nstd::cout << \"Roots found: \" << roots << std::endl;\n```\n\nThe background of our root finder is to do a first order [Taylor expansion near x_hat](https://mathinsight.org/taylors_theorem_multivariable_introduction).\n\nWe then have four steps:\n\n1. Get f(x_hat)\n\n2. Get J(x_hat) -> Jacobian\n\n3. Solve for delta: J(x_hat)(delta) = -f(x_hat)\n\n4. Update: x_hat = x_hat + delta\n\n### 6.8 Dual Number Extension\n\nAs a side-mini project, we sort of implemented dual numbers. We have overloaded the basic mathematical operators, but not done much outside of that.\n\nIt is fairly simple to understand.\n\nHere is an example of how to use:\n```\n# make a dual number\nDual y;\nDual y1(3.9,9.3);\n\n# some operations\ny1 = 2.9;\ny1 *= 2.8;\ny1 /= 2.8;\ny += y1;\n\n# some math \nDual x(-0.5,4.0);\nDual asinx = asin(x);\nDual cosx = cos(x);\nDual powx = pow(x, y);\n\n# get real and get dual values\nreal_val = powx.real();\ndual_val = powx.dual()\n```","metadata":{"tags":[],"cell_id":"00003-9c061f16-d963-4ac8-aa46-de27c926525a","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"![Picture title](image-20201212-114633.png)","metadata":{"tags":[],"cell_id":"00004-6d605e2d-f872-4f83-b8b5-85353acc8730","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-1e1a4131-c14f-46c3-b449-5c4e8db302ea","output_cleared":false,"source_hash":"52fc8de2","execution_millis":7,"execution_start":1607792276431,"deepnote_cell_type":"code"},"source":"from IPython.display import HTML, display\n\ndata = [[\"\",\"Assign\",\"Warning\",\"Error\"],\n        [\"Calculation Result & Single ADFunc Input, e.g. sin(x),tan(x),sqrt(x)\",\"Assign to return\",\"\",\"\",\"\"],\n        [\"Calculation Result & Two ADFunc Inputs, e.g. A+B, pow(A,B)\",\"Assign to return only when A and B have same names\",\"\",\"1. Only one of A and B has names, 2. A and B have different names\"],\n        [\"Calculation Result & One ADFunc Inputs & One Scalar, e.g. A+3, pow(3,A)\",\"A has names\",\"\",\"\"],\n        [\"Calculation and Assign & Single ADFunc Input, e.g. A+=B，A/=B\",\"Assign LHS only\",\"LHS has name but RHS does not\",\"A and B have different names\"],\n        [\"Comparison Operators, e.g. ==,<=,>\",\"\",\"One of A and B has names, one does not\",\"A and B have different names\"]\n        ]\n\n\ndisplay(HTML(\n   '<table><tr>{}</tr></table>'.format(\n       '</tr><tr>'.join(\n           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n       )\n))","execution_count":1,"outputs":[{"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table><tr><td></td><td>Assign</td><td>Warning</td><td>Error</td></tr><tr><td>Calculation Result & Single ADFunc Input, e.g. sin(x),tan(x),sqrt(x)</td><td>Assign to return</td><td></td><td></td><td></td></tr><tr><td>Calculation Result & Two ADFunc Inputs, e.g. A+B, pow(A,B)</td><td>Assign to return only when A and B have same names</td><td></td><td>1. Only one of A and B has names, 2. A and B have different names</td></tr><tr><td>Calculation Result & One ADFunc Inputs & One Scalar, e.g. A+3, pow(3,A)</td><td>A has names</td><td></td><td></td></tr><tr><td>Calculation and Assign & Single ADFunc Input, e.g. A+=B，A/=B</td><td>Assign LHS only</td><td>LHS has name but RHS does not</td><td>A and B have different names</td></tr><tr><td>Comparison Operators, e.g. ==,<=,></td><td></td><td>One of A and B has names, one does not</td><td>A and B have different names</td></tr></table>"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","source":"### 6.7 Vectorized Input\n \nIf you don't want to create seed vectors every time, it doesn't matter! We can do it fot you and setting\nseed vectors by default to unit vectors and the order of variables in the seed vector follows \nthe order you put initial values. You can easily use set_seed() to change seed vector afterwards!\n\nExample: (x1,...,xn) vector inputs with a single function f(x1,..., xn)\n\nf.dval is a vector with two elements, $\\left[df/dx_1, df/dx_2, ...\\right]$\n\n``` \n    #include \"ADFunc.h\"\n    #include \"ADFuncVector.h\"\n    #include <math.h>\n    #include <typeinfo>\n    #include <iostream>\n    #include <string>\n    #include <cmath>\n\n    using namespace std;\n\n    vector<double> init_values = {5.0, 3.0};\n    vector<ADFunc> multi_vars = multiVar(init_values);\n    ADFunc x = multi_vars[0];\n    ADFunc y = multi_vars[1];\n    f = pow((y*x), 2) + 7*sin(log(x));\n\n    vector<double> dvals = f.dval_wrt(vector<int> {0,1});\n    cout << \"df/dx: \" << dvals[0] << endl; //89.9459\n    cout << \"df/dy: \" << dvals[1] << endl; //150\n    cout << \"f2 val: \" << f.val() << \"\\n\" << endl; //231.995\n```\n### 6.8 Construct ADFuncVector\n\nADFuncVector represents a vector of functions and performs automatic differentiation on this \nvectorized functions as a whole. It is a wrapper class of std::vector<ADFunc> to make taking\npartial derivative from vectorized functions easier (we write the for-loops of .dval_wrt()\nfor you so all you need to do is declaring a ADFuncVector and calling its member functions)!\nYou have many options of taking partial derivatives of a ADFuncVector object, whether you want\na scalar number, a 1-d vector, or a 2-D matrix. We also support a function of getting Jacobian matrix \nfor your convenience. \n\nWhen x is a vector $\\left[x_1, x_2, ..., x_n\\right]$, $F(x)$ is a vector of functions $\\left[f_1(x), f_2(x), ..., f_m(x)\\right]$\n\nf.der is a matrix with self defined size. It is handy to use ADFuncVector class. \n\n``` \n    #include \"ADFunc.h\"\n    #include \"ADFuncVector.h\"\n    #include <math.h>\n    #include <typeinfo>\n    #include <iostream>\n    #include <string>\n    #include <cmath>\n\n    using namespace std;\n\n    std::vector<double> init_values = {5.0,3.0,0.3};\n    std::vector<ADFunc> multi_vars = multiVar(init_values);\n    ADFunc x = multi_vars[0];\n    ADFunc y = multi_vars[1];\n    ADFunc z = multi_vars[2];\n    F = {2*pow(x,z)/y, 5*sin(y)*x+z/sin(y)*sinh(y), 9*sinh(z)-exp(x*y)};\n    ADFuncVector Fvec = ADFuncVector(F);\n    vector<vector<std::pair<int, int> > > fun_var_index_m = { {{0,0},{2,1},{0,2}},{{1,0},{1,1},{2,2}}};\n\n    vector<vector<double> > dval_m = Fvec.dval_wrt(fun_var_index_m);\n    cout << \"df1/dx1: \" << dval_m[0][0] << endl; \n    cout << \"df1/dx2: \" << dval_m[0][1] << endl; // -1.63450869e+07\n    cout << \"df1/dx3: \" << dval_m[0][2] << endl; // 1.7389\n    cout << \"df2/dx1: \" << dval_m[1][0] << endl; // 0.7056\n    cout << \"df2/dx2: \" << dval_m[1][1] << endl; // 146.053\n    cout << \"df2/dx3: \" << dval_m[1][2] << endl; // 9.40805\n    cout << \"val for f1 in F: \" << Fvec.val(0) << endl; // 1.08\n    cout << \"val for f2 in F: \" << Fvec.val(1) << endl; // 24.8245\n    cout << \"val for f3 in F: \" << Fvec.val(2) << endl; // -3269014.63\n```\n\nThere are many ways to do .dval_wrt() using ADFuncVector class! Please checkout Doxygen. ","metadata":{"tags":[],"cell_id":"00004-a831aa04-d799-4605-b2e7-85a036b8eb80","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n","metadata":{"tags":[],"cell_id":"00004-52c24218-a800-4b33-9dc8-3136b4d0c26f","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 7. For Developers\n\n### 7.1 config.sh\n\nThere are 4 config.sh files the whole repo and they locate at: `.` , `\\AutoDiff`, `\\Dual`, `\\App`. The config.sh in\nthe root directory, once triggered (using `./config.sh` command ) will recursively all other 3 config.sh if\ncertain condition is met. We will talk about each of these as follows.\n\n#### 7.1.1 config.sh in the root directory\n\nThere are 6 operations flag in line 6-11. By default, all of them are set to be 0. \n\n`BUILD_3PL=0`: build 3rd party library. 0 means building 3rd party library.\n\n`BUILD_LIB=0`: build AutoDiff library and Dual library. 0 means build AutoDiff library and Dual library.\n\n`BUILD_APP=0`: build App library, which are the applications using AutoDiff and Dual libraries. \n0 means not building App library.\n\n`BUILD_TYPE=0`: compile the project in optimized mode or debug mode. 0 means optimized mode.\n\n`CLEAN_DIST=0`:\n\n`CLEAN=0`:\n\nYou can use execute option to manipulate these operation: \n\n`./config.sh -3pl`: build the 3rd party libraries: gtest, lcov.\n\n`./config.sh -lib`: build AutoDiff and Dual libraries. You can also use `./config.sh --library`\n\n`./config.sh -app`: build the App which uses AutoDiff, Dual libraries. You can also use `./config.sh --app`\n\n`./config.sh -c`:  removes local build directories. You can also use `./config.sh --clean`\n\n`./config.sh -dc`: removes builds and install directories. You can also use `./config.sh --distclean`\n\n`./config.sh -opt`:  compile the project in optimized mode. You can also use `./config.sh --release`\n\n`./config.sh -deb`: compile the project in debug mode. You can also use `./config.sh --debug`\n\n`./config.sh -ton`:  turn on unit tests (google tests). You can also use `./config.sh --testson` \n\nMore generally, you can use `./config.sh -h` or `./config.sh --help` to see these options.\n\n\n#### 7.1.2 AutoDiff/config.sh\n\nTo code in AutoDiff folder, you always run `./config.sh` to build and test it out your code. `./config.sh` will\ngenerate two folders, build and install, if they are not already there. In install folder,\n\n```\n- install\n\n    - include\n\n        *.h\n\n    - lib\n\n        lib*.so\n```\n\nthere are header files and share library in .so format.\n\n#### 7.1.3 App/config.sh (same cases for Dual/config.sh)\n\nThere is not testing in App folder. All the outputs are given by std::cout. \n\nApp folder uses dynamic library to link .cpp files in App with AutoDiff library.\nCurrently CMakeLists.txt doesn't specify an absolute path, \nit only tells compiler some suggested locations for it to look for AutoDiff .so library.\nSo, it is possible that the linkage cannot work properly.\nIt is advised that before running `./config.sh` you clean build and install in App folder, \nrebuild the library in AutoDiff or Dual, and then `./config.sh` in App folder to avoid some depreciated files being\npicked up.\n\n### 7.2 Code coverage\n\nTo get code coverage, you can make use of coverage.sh, which locate at both AutoDiff and Dual folder.\nIt is advised that you delete build and install folders first, \nor go into build folder,\ndo `make clean` before running `./coverage.sh`\n\nCurrently, there 4 testfiles using GoogleTest in AutoDiff folder:\n\n\n> **test_ADF_unit.cpp**: unit test for ADFunc class with the main focus on computing funality.\n\n> **test_ADF_nested.cpp**: unit test for ADFunc class with the main focus on nested operation (e.g. cos(pow(x,2.0))) and vectorized input.\n\n> **test_ADF_name.cpp**: unit test for ADFunc class with the main focus on name mode.\n\n> **test_ADFV_unit.cpp**: unit test for ADFuncVector class.\n\n","metadata":{"tags":[],"cell_id":"00003-4a4abd1e-2d1a-4969-8ef2-75c3177560dd","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 7.3 Possible Error Messages\n\n**1. Cannot merge previous GCDA file: corrupt arc tag (0x00000000)**\n\nIf you see this error when you want to see code coverage, it probably relates to the previously\ngenerated coverage, build, or install folder being used to generate code coverage but they are not \ncompatible with your current code. To solve this, you can delete these folders and rerun `./config.sh` \nand then `./coverage.sh`. \n\n**2. Permission denied **\n\nThis error relates to permission denied to access or execute a file. \nFirst, you would need to check if the file being executed exist in the directory. \nSecond, you may use `chmod +x filename` to make the file executable\nThird, you can try adding sudo in front of the command.\n\n**3. '\\r': command not found**\n\nThis is a possible error on a WSL like Ubuntu for Windows. \nTo solve this, first get `dos2unixsudo` using `sudo apt-get install dos2unix`.\nThen for each of the seven .sh files (config.sh, coverage.sh, or build_3PL.sh), run `dos2unix config.sh`, `dos2unix coverage.sh`, and `dos2unix build_3PL.sh`.\nReturn to the final project directory and run `./config.sh`.\n\n**4. cmake: command not found**\n\nYou need to install CMake package.\n\nIn Windows Ubuntu: `sudo apt-get install cmake`\n\nIf you have brew in Mac, `brew install cmake`\n\nIn Linux: see this useful link \n\nhttps://geeksww.com/tutorials/operating_systems/linux/installation/downloading_compiling_and_installing_cmake_on_linux.php\n\n**5. lcov: command not found**\n\nYou need to install lcov packge. We have the package already downloaded for you!\nTo fix this, enter the `3PL/lcov` folder and \n`make install` (or `sudo make install` if you get an \"permission denied\" error).\nIf you have `'\\r': command not found` errors, you will have `dos2unix` several `.sh` files in the \n`3PL/lcov/bin` directory.\n\n### 7.4 Operating system: \n\n- MacOS\n\nCurrent \"Dual/CMakeLists.txt\" and \"AutoDiff/CMakeLists.txt\" comment out two pieces that make it different from\nLinux version. \n\nThe 1st one is \"line 10: set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib)\". If you receive an error like \"realpath: command not found\", you should consider line10 out.\n\nThe 2nd one is \"line 27: if(CMAKE_C_COMPILER_ID MATCHES \"GNU\")\" and correspondingly \"line 30: endif()\". It maybe due to that MacOS use LLVM gcov to emulate gcov, if MacOS users don't commit these two lines out, the unittesting won't be triggered. Therefore, a code coverage report website cannot be generated.\n\n- Linux\n\nUncomment the two parts (line10, line27, line30) and it should work. \n\n## 8. Contact Us/ Pull Requests\n\nIf you need any help using our library or desire to provide feedback (which we are always very eager for!) contact us here:\n- Scarlett Gong: wenlin_gong@g.harvard.edu \n- Morris Reeves: morrisreeves@g.harvard.edu \n- Gayatri Balasubramanian: gayatrib@college.harvard.edu\n\n## 9. Future\n\nWe were most inspired by the questions of what motivates AD, and how to make our work more accessible to peopleof all backgrounds, since that is at the core our experience as amateur coders.\n\n- On the interactability side, we were inspired by the Heroku App, because that took AD to an audience beyond coders. Not only that, but it made the experience of working with a programming language math library so much more enjoyable. We want to add some sort of interactive, trasnparent interface that takes our library beyond the scope of those who work in C++.\n\n- Second, while root finding looks for where a function crosses the x-axis, we wanted to take that concept and appy it to the first defivative of a function, thereby giving us the local min and max ofthe function. We always wanted to expant that to the the second derivative test, which would tell us concavity and the second-derivative test. We were talking about AD's rise with deep learning, neural networks, and gradient descent. Core to that is the ability to to apply the first and second derivative test to solve optimization problems.\n\n- Lastly, we spent time in this course working with plotting. Another way to make our application more accessible would be to have a graphical way to plot a function and the tangent line at a point. For this, we have everything we need in terms of evaluating the derivative at a point. All we would need is to have a scale (like in Desmos for example), and to plot the result of our derivative and evaluation at a point.\n\n## 10. Video and Powerpoint\n\nHere is [our video](https://youtu.be/aPU9cub6_C4) and our [powerpoint slides](https://docs.google.com/presentation/d/17yIb86cNVVqYNiNMmrM6RLtlrbp9zTTsFMUs1W6eafk/edit?usp=sharing).","metadata":{"tags":[],"cell_id":"00004-5fd60b41-accc-487f-813b-cd44aa940826","output_cleared":false,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"7c34ee77-cfd3-447b-b4ab-b12b37be29a7","deepnote_execution_queue":[]}}